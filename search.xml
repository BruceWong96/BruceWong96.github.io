<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数仓篇-1-实时数仓架构演进之路</title>
      <link href="/2021/11/02/shu-cang-pian-1-shi-shi-shu-cang-jia-gou-yan-jin-zhi-lu/"/>
      <url>/2021/11/02/shu-cang-pian-1-shi-shi-shu-cang-jia-gou-yan-jin-zhi-lu/</url>
      
        <content type="html"><![CDATA[<h1 id="实时数仓架构演进之路"><a href="#实时数仓架构演进之路" class="headerlink" title="实时数仓架构演进之路"></a>实时数仓架构演进之路</h1><h2 id="大数据处理现状"><a href="#大数据处理现状" class="headerlink" title="大数据处理现状"></a>大数据处理现状</h2><p>当前基于 Hive 的离线数据仓库已经非常成熟稳定，数据中台体系也基本上围绕离线数仓进行建设。但是随着实时计算引擎的不断发展以及业务对于实时报表的产出需求不断膨胀，业界最近几年就一直聚焦并探索与两个相关的热点问题：实时数仓建设和大数据架构的批流一体建设。</p><h2 id="实时数仓-1-0"><a href="#实时数仓-1-0" class="headerlink" title="实时数仓 1.0"></a>实时数仓 1.0</h2><p>传统意义上我们通常将数据处理分为实时数据处理和离线数据处理。对于实时处理场景，我们一般又可以分为两类，一类诸如监控告警类、大屏展示类场景要求秒级甚至亚秒级；另一类诸如大部分实时报表的需求通常没有非常高的时效性要求，一般分钟级别，比如 10 分钟甚至 30 分钟以内都可以接受。</p><p>对于第一类实时数据场景来说，业界通常的做法比较简单粗暴，一般也不需要非常仔细地进行数据分层，数据直接通过 Flink 计算或者聚合之后将结果写入 MySQL/ES/HBase/Druid/Kudu 等，直接提供应用查询或者多维分析。如下所示：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-2/1635853428370-image.png" alt="实时数仓 0.0"></p><p>而对于后者来说，通常做法会按照数仓结构进行设计，我们称后者这种应用场景为**实时数仓，将作为本篇文章讨论的重点。从业界情况来看，当前主流的实时数仓架构基本都是基于 Kafka+Flink 的架构（为了行文方便，就称为实时数仓 1.0 ）。下图是基于业界各大公司分享的实时数仓架构抽象的一个方案：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-2/1635853462518-image_1.png" alt="实时数仓 1.0"></p><p>这套架构总体依然遵循标准的数仓分层结构，各种数据首先汇聚于 ODS 数据接入层。再接着经过这些来源明细数据的数据清洗、过滤等操作，完成多来源同类明细数据的融合，形成面向业务主题的DWD数据明细层。在此基础上进行轻度的汇总操作，形成一定程度上方便查询的 DWS 轻度汇总层（注：这里没有画出DIM维度层，一般选型为 Redis/HBase ，下文架构图中同样没有画出DIM维度层，在此说明）。最后再面向业务需求，在 DWS 层基础上进一步对数据进行组织进入 ADS 数据应用层，业务在数据应用层的基础上支持用户画像、用户报表等业务场景。</p><p>基于 Kafka+Flink 的这套架构方案很好的解决了实时数仓对于时效性的业务诉求，通常延迟可以做到秒级甚至更短。基于上图所示实时数仓架构方案，笔者整理了一个目前业界比较主流的整体数仓架构方案：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-2/1635853481195-image_2.png" alt="业界整体数仓"></p><p>上图中上层链路是离线数仓数据流转链路，下层链路是实时数仓数据流转链路，当然实际情况可能是很多公司在实时数仓建设中并没有严格按照数仓分层结构进行分层，与上图稍有不同。</p><p>然而基于 Kafka+Flink 的实时数仓方案有几个非常明显的缺陷：</p><p>（1）Kafka 无法支持海量数据存储。对于海量数据量的业务线来说，Kafka 一般只能存储非常短时间的数据，比如最近一周，甚至最近一天；</p><p>（2）Kafka 无法支持高效的 OLAP 查询。大多数业务都希望能在 DWD\DWS 层支持即席查询的，但是 Kafka 无法非常友好地支持这样的需求；</p><p>（3）无法复用目前已经非常成熟的基于离线数仓的数据血缘、数据质量管理体系。需要重新实现一套数据血缘、数据质量管理体系；</p><p>（4）Lambad 架构维护成本很高。很显然，这种架构下数据存在两份、schema 不统一、 数据处理逻辑不统一，整个数仓系统维护成本很高；</p><p>（5）Kafka 不支持 update/upsert。目前 Kafka 仅支持 append。实际场景中在 DWS 轻度汇聚层很多时候是需要更新的，DWD 明细层到 DWS 轻度汇聚层一般会根据时间粒度以及维度进行一定的聚合，用于减少数据量，提升查询性能。假如原始数据是秒级数据，聚合窗口是 1 分钟，那就有可能产生某些延迟的数据经过时间窗口聚合之后需要更新之前数据的需求。这部分更新需求无法使用 Kafka 实现。</p><blockquote><p>注意：第 (5) 点 Flink 1.12 官方提供了 upsert-kafka connector 具体实现可以看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html">官网</a>，这篇文章在写的时候可能还没有，保留是为了解释 upsert-kafka 的必要性。</p></blockquote><p>所以实时数仓发展到现在的架构，一定程度上解决了数据报表时效性问题，但是这样的架构依然存在不少问题，随着技术的发展，相信基于 Kafka+Flink 的实时数仓架构也会进一步往前发展。那会往哪里发展呢？</p><p><strong>大数据架构的批流一体建设。</strong> </p><p>带着上面的问题我们再来接着聊一聊最近一两年和实时数仓一样很火的另一个概念：批流一体。对于批流一体的理解，笔者发现有很多种解读，比如有些业界前辈认为批和流在开发层面上都统一到相同的 SQL 上是批流一体，又有些前辈认为在计算引擎层面上批和流可以集成在同一个计算引擎是批流一体，比如 Spark/Spark Structured Streaming就算一个在计算引擎层面实现了批流一体的计算框架，与此同时另一个计算引擎 Flink，目前在流处理方面已经做了很多的工作而且在业界得到了普遍的认可，但在批处理方面还有一定的路要走。</p><h2 id="实时数仓-2-0"><a href="#实时数仓-2-0" class="headerlink" title="实时数仓 2.0"></a>实时数仓 2.0</h2><p>笔者认为无论是业务 SQL 使用上的统一还是计算引擎上的统一，都是批流一体的一个方面。除此之外，批流一体还有一个最核心的方面，那就是存储层面上的统一。在这个方面业界也有一些走在前面的技术，比如最近一段时间开始流行起来的数据湖三剑客 – delta/hudi/iceberg，就在往这个方向走。存储一旦能够做到统一，上述数据仓库架构就会变成如下模样（以Iceberg数据湖作为统一存储为例），称为实时数仓 2.0：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-2/1635853531630-image_3.png" alt="实时数仓 2.0"></p><p>这套架构中无论是流处理还是批处理，数据存储都统一到数据湖 Iceberg 上。那这么一套架构将存储统一后有什么好处呢？很明显，可以解决 Kafka+Flink 架构实时数仓存在的前面4个问题：</p><p>（1）可以解决 Kafka 存储数据量少的问题。目前所有数据湖基本思路都是基于 HDFS 之上实现的一个文件管理系统，所以数据体量可以很大。</p><p>（2）DW层数据依然可以支持 OLAP 查询。同样数据湖基于 HDFS 之上实现，只需要当前的 OLAP 查询引擎做一些适配就可以进行 OLAP 查询。</p><p>（3）批流存储都基于 Iceberg/HDFS 存储之后，就完全可以复用一套相同的数据血缘、数据质量管理体系。</p><p>（4）Kappa 架构相比 Lambad 架构来说，schema 统一，数据处理逻辑统一，用户不再需要维护两份数据。</p><p>有的同学说了，这不，你直接解决了前 4 个问题嘛，还有第 5 个问题呢？对，第 5 个问题下文会讲到。</p><p>又有的同学会说了，上述架构确实解决了 Lambad 架构的诸多问题，但是这套架构看起来就像是一条离线处理链路，它是怎么做到报表实时产出呢？确实，上述架构图主要将离线处理链路上的 HDFS 换成了数据湖 Iceberg，就号称可以实现实时数仓，听起来容易让人迷糊。这里的关键就是数据湖 Iceberg，它到底有什么魔力？</p><p>为了回答这个问题，笔者就上述架构以及数据湖技术本身做一个简单的介绍（接下来也会基于 Iceberg 出一个专题深入介绍数据湖技术）。上述架构图中有两条数据处理链路，一条是基于 Flink 的实时数据链路，一条是基于 Spark 的离线数据链路。通常数据都是直接走实时链路处理，而离线链路则更多的应用于数据修正等非常规场景。这样的架构要成为一个可以落地的实时数仓方案，数据湖 Iceberg 是需要满足如下几个要求的：</p><p>（1）支持流式写入-增量拉取。流式写入其实现在基于 Flink 就可以实现，无非是将 checkpoint 间隔设置的短一点，比如 1 分钟，就意味每分钟生成的文件就可以写入到 HDFS，这就是流式写入。没错，但是这里有两个问题，第一个问题是小文件很多，但这不是最关键的，第二个问题才是最致命的，就是上游每分钟提交了很多文件到 HDFS 上，下游消费的 Flink 是不知道哪些文件是最新提交的，因此下游 Flink 就不知道应该去消费处理哪些文件。这个问题才是离线数仓做不到实时的最关键原因之一，离线数仓的玩法是说上游将数据全部导入完成了，告诉下游说这波数据全部导完了，你可以消费处理了，这样的话就做不到实时处理。</p><p>数据湖就解决了这个问题。实时数据链路处理的时候上游 Flink 写入的文件进来之后，下游就可以将数据文件一致性地读走。这里强调一致性地读，就是不能多读一个文件也不能少读一个文件。上游这段时间写了多少文件，下游就要读走多少文件。我们称这样的读取叫增量拉取。</p><p>（2）解决小文件多的问题，数据湖实现了相关合并小文件的接口，Spark/Flink 上层引擎可以周期性地调用接口进行小文件合并。</p><p>（3）支持批量以及流式的 Upsert(Delete) 功能。批量 Upsert/Delete 功能主要用于离线数据修正。流式 upsert 场景上文介绍了，主要是流处理场景下经过窗口时间聚合之后有延迟数据到来的话会有更新的需求。这类需求是需要一个可以支持更新的存储系统的，而离线数仓做更新的话需要全量数据覆盖，这也是离线数仓做不到实时的关键原因之一，数据湖是需要解决掉这个问题的。</p><p>（4）支持比较完整的 OLAP 生态。比如支持 Hive/Spark/Presto/Impala 等 OLAP 查询引擎，提供高效的多维聚合查询性能。</p><p>这里需要备注一点，目前 Iceberg 部分功能还在开发中。具体技术层面 Iceberg 是怎么解决上述问题的，还需要持续关注社区。</p><h2 id="实时数仓-3-0"><a href="#实时数仓-3-0" class="headerlink" title="实时数仓 3.0"></a>实时数仓 3.0</h2><p>按照批流一体上面的探讨，如果计算引擎做到了批流一体的统一，就可以做到SQL统一、计算统一以及存储统一，这时就迈入实时数仓 3.0 时代。对于以 Spark 为核心技术栈的公司来说，实时数仓 2.0 的到来就意味着 3.0 的到来，因为在计算引擎层面 Spark 早已做到批流一体。基于 Spark 数据湖的 3.0 架构如下图：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-2/1635853556445-image_4.png" alt="实时数仓 3.0 Spark"></p><p>假如未来Flink在批处理领域成熟到一定程度，基于Flink 数据湖的3.0架构如下图：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-2/1635853563224-image_5.png" alt="实时数仓 3.0 Flink"></p><p>上面所介绍的，是笔者认为接下来几年数据仓库发展的一个可能路径。对于业界目前实时数仓的一个发展预估，个人觉得目前业界大多公司都还往实时数仓 1.0 这个架构上靠；而在接下来1到2年时间随着数据湖技术的成熟，实时数仓 2.0 架构会成为越来越多公司的选择，其实到了 2.0 时代之后，业务同学最关心的报表实时性诉求和大数据平台同学最关心的数据存储一份诉求都可以解决；随着计算引擎的成熟，实时数仓 3.0 可能和实时数仓 2.0 一起或者略微滞后一些普及。</p><p>计算、存储、分析一直是实时数仓发展的基石。</p><h2 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h2><p>子和，网易大数据开发工程师，长期从事分布式KV数据库、分布式时序数据库以及大数据底层组件等相关工作。</p><p>本文参考：<a href="https://mp.weixin.qq.com/s/lt7KZ_vs_RsI9VTV89TusQ#at">https://mp.weixin.qq.com/s/lt7KZ_vs_RsI9VTV89TusQ#at</a></p>]]></content>
      
      
      <categories>
          
          <category> 数仓篇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> 实时数仓 </tag>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink-原理篇-1-Flink-架构和拓扑概览</title>
      <link href="/2021/11/01/flink-yuan-li-pian-1-flink-jia-gou-he-tuo-bu-gai-lan/"/>
      <url>/2021/11/01/flink-yuan-li-pian-1-flink-jia-gou-he-tuo-bu-gai-lan/</url>
      
        <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>了解一个系统，一般都是从架构开始。我们关心的是一个系统启动后，各个节点都启动了哪些服务，每个服务的作用是什么，各个服务之间是怎么交互和协调的。下方是 Flink 集群启动后的架构图。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-1/1635766724607-image.png" alt="Flink Architecture"></p><p>当 Flink 集群启动后，会首先启动一个 JobManager 和 一个或多个 TaskManager 。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。</p><ul><li><p>Client 为提交 Job 的客户端，可以是运行在任何机器上（只要与 JobManager 网络环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming 的任务），也可以不结束并等待结果返回。</p></li><li><p>JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 Jar 包资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</p></li><li><p>TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</p></li></ul><h2 id="组件详细介绍"><a href="#组件详细介绍" class="headerlink" title="组件详细介绍"></a>组件详细介绍</h2><h3 id="JobManager"><a href="#JobManager" class="headerlink" title="JobManager"></a>JobManager</h3><p><em>JobManager</em>  具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：</p><ul><li><strong>ResourceManager</strong> </li></ul><p>    - <em>ResourceManager</em>  负责 Flink 集群中的资源提供、回收、分配 - 它管理 <strong>task slots</strong> ，这是 Flink 集群中资源调度的单位。Flink 为不同的环境和资源提供者（例如 YARN、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。</p><ul><li><strong>Dispatcher</strong> </li></ul><p>    - <em>Dispatcher</em>  提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。</p><ul><li>JobMaster</li></ul><p>    - JobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。</p><p>至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。</p><h2 id="TaskManagers"><a href="#TaskManagers" class="headerlink" title="TaskManagers"></a>TaskManagers</h2><p>TaskManager（也称为 <em>worker</em> ）执行作业流的 task，并且缓存和交换数据流。</p><p>必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task <em>slot</em> 。TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子（请参考<a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/concepts/flink-architecture/#tasks-and-operator-chains">Tasks 和算子链</a>）。</p><h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><p>什么是 Flink Job ？官方术语解释如下：</p><p>A Flink Job is the runtime representation of a <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html#logical-graph">logical graph</a> (also often called dataflow graph) that is created and submitted by calling <code>execute()</code> in a <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html#flink-application">Flink Application</a>.</p><p>Flink Job 是一个逻辑图（也叫做数据流图）在运行时中的表示，在 Flink 应用中调用 execute() 函数时会创建并且提交 Flink Job。</p><p>简单理解就是，我们写的业务代码都可以叫做 Flink Job。Flink 会把提交的 Job 解析成 Graph（图）来执行。</p><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><p>Flink 中的图可以分为如下四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-1/1635766765275-flink-graph.png" alt="Flink Graph"></p><h3 id="Stream-Graph"><a href="#Stream-Graph" class="headerlink" title="Stream Graph"></a>Stream Graph</h3><p>根据用户通过 Stream API 编写的代码生成的最初的图</p><ul><li><p>StreamNode：用来表示 operator 的类，并具有所有相关的属性，如并发度，入边和出边等。</p></li><li><p>StreamEdge：表示连接两个 StreamNode 的边</p></li></ul><p>StremGraph 是可以查看的，比如平时我们写的业务代码如下：</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public static void main(String[] args) throws Exception {  // 检查输入  final ParameterTool params = ParameterTool.fromArgs(args);  ...  // set up the execution environment  final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  // get input data  DataStream&lt;String&gt; text =      env.socketTextStream(params.get("hostname"), params.getInt("port"), '\n', 0);  DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts =      // split up the lines in pairs (2-tuples) containing: (word,1)      text.flatMap(new Tokenizer())          // group by the tuple field "0" and sum up tuple field "1"          .keyBy(0)          .sum(1);  counts.print();    // execute program  env.execute("WordCount from SocketTextStream Example");}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>把最后一行代码 <code>env.execute </code> 替换成 <code>System.out.println(env.getExecutionPlan());</code> 并在本地运行该代码（并发度设置为2 ），可以得到该拓扑结构的 StreamGraph 的 JSON 串，将该 JSON 串粘贴到 <a href="http://flink.apache.org/visualizer/">http://flink.apache.org/visualizer/</a> 中，可以可视化该图，类似下面这样。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-1/1635766817275-image_1.png" alt="StreamGraph"></p><h3 id="Job-Graph"><a href="#Job-Graph" class="headerlink" title="Job Graph"></a>Job Graph</h3><p>StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</p><ul><li><p>JobVertex：经过优化后符合条件的多个 StreamNode 可能会 Chain 在一起生成一个 JobVertex，即一个 JobVertex 包含一个或多个 operator，JobVertex 的输入是 JobEdge，输出是 IntermediateDataSet。</p></li><li><p>IntermediateDataSet：表示 JobVertex 的输出，即经过 operator 处理产生的数据集。 producer 是 JobVertex，consumer 是 JobEdge。</p></li><li><p>JobEdge：代表了 job graph 中的一条数据传输通道，source 是 IntermediateDataSet，target 是 JobVertex。即数据通过 JobEdge 由 IntermediateDataSet 传递至目标 JobVertex。</p></li></ul><p>JobGraph 也是可以查看的，我们在集群上的 WebUI 所看到的图就是 JobGraph，如下图。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-1/1635766836270-image_2.png" alt="JobGraph"></p><h3 id="Execution-Graph"><a href="#Execution-Graph" class="headerlink" title="Execution Graph"></a><strong>Execution Graph</strong></h3><p>JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。JobManager 中的 JobMaster 就是根据 ExecutionGraph 来进行调度的。</p><ul><li><p>ExecutionJobVertex：和JobGraph中的JobVertex一一对应。每一个ExecutionJobVertex都有和并发度一样多的 ExecutionVertex。</p></li><li><p>ExecutionVertex：表示ExecutionJobVertex的其中一个并发子任务，输入是ExecutionEdge，输出是IntermediateResultPartition。</p></li><li><p>IntermediateResult：和JobGraph中的IntermediateDataSet一一对应。一个IntermediateResult包含多个IntermediateResultPartition，其个数等于该operator的并发度。</p></li><li><p>IntermediateResultPartition：表示ExecutionVertex的一个输出分区，producer是ExecutionVertex，consumer是若干个ExecutionEdge。</p></li><li><p>ExecutionEdge：表示ExecutionVertex的输入，source是IntermediateResultPartition，target是ExecutionVertex。source和target都只能是一个。</p></li><li><p>Execution：是执行一个 ExecutionVertex 的一次尝试。当发生故障或者数据需要重算的情况下 ExecutionVertex 可能会有多个 ExecutionAttemptID。一个 Execution 通过 ExecutionAttemptID 来唯一标识。JM和TM之间关于 task 的部署和 task status 的更新都是通过 ExecutionAttemptID 来确定消息接受者。</p></li></ul><h3 id="Physical-Graph"><a href="#Physical-Graph" class="headerlink" title="Physical Graph"></a>Physical Graph</h3><p>JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构（源码中没有这个类）。</p><ul><li><p>Task：Execution被调度后在分配的 TaskManager 中启动对应的 Task。Task 包裹了具有用户执行逻辑的 operator。</p></li><li><p>ResultPartition：代表由一个Task的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应。</p></li><li><p>ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费 Task 数和 DistributionPattern 来决定。</p></li><li><p>InputGate：代表Task的输入封装，和JobGraph中JobEdge一一对应。每个InputGate消费了一个或多个的ResultPartition。</p></li><li><p>InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出。</p></li></ul><hr><p>具体四张图的流程可以看下图</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-11-1/1635766869755-image_3.png" alt="Flink Graph"></p><p>那么 Flink 为什么要设计这4张图呢，其目的是什么呢？Spark 中也有多张图，数据依赖图以及物理执行的DAG。其目的都是一样的，就是解耦，每张图各司其职。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/">http://wuchong.me/blog/2016/05/03/flink-internals-overview/</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html">https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html</a></p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/concepts/flink-architecture/">https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/concepts/flink-architecture/</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink 原理篇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Flink 源码篇-1] Flink 1.12 命令行提交 Job 的流程解析</title>
      <link href="/2021/10/26/flink-yuan-ma-pian-1-flink-1-12-ming-ling-xing-ti-jiao-job-de-liu-cheng-jie-xi/"/>
      <url>/2021/10/26/flink-yuan-ma-pian-1-flink-1-12-ming-ling-xing-ti-jiao-job-de-liu-cheng-jie-xi/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-10-26/1635259498061-image.png" alt="Flink"></p><p>本文主要介绍 on yarn 模式下从命令行提交任务的执行流程，从运行 run 到执行用户程序的 main() 函数，由于转换为 JobGraph 的流程十分复杂，所以放到另一篇文章单独写。</p><h3 id="启动命令"><a href="#启动命令" class="headerlink" title="启动命令"></a>启动命令</h3><p>用户通过 DataStream API 构建 Flink 应用程序之后，下一步就是将构建好的 JAR 包提交到集群中运行，整个过程涉及客户端和集群运行时之间的交互。</p><p>提交代码如下</p><pre class="line-numbers language-SQL" data-language="SQL"><code class="language-SQL">${FLINK_HOME}/bin/flink run application.jar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="启动脚本追溯"><a href="#启动脚本追溯" class="headerlink" title="启动脚本追溯"></a>启动脚本追溯</h3><p>脚本最后一行</p><pre class="line-numbers language-SQL" data-language="SQL"><code class="language-SQL"># Add HADOOP_CLASSPATH to allow the usage of Hadoop file systemsexec $JAVA_RUN $JVM_ARGS $FLINK_ENV_JAVA_OPTS "${log_setting[@]}" -classpath "`manglePathList "$CC_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS"`" org.apache.flink.client.cli.CliFrontend "$@"# "$@" 表示将传递给脚本的所有参数 传递给 CliFrontend# 所以 CliFrontend 就是 入口<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="源代码溯源"><a href="#源代码溯源" class="headerlink" title="源代码溯源"></a>源代码溯源</h3><p>从脚本中的路径可以看出这个类在 <code>flink-clients</code> 模块，<code>CliFrontend</code> 类的主函数如下，从主函数来看整个提交流程。</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public static void main(final String[] args) {    EnvironmentInformation.logEnvironmentInfo(LOG, "Command Line Client", args);    // 1. find the configuration directory    // 这一步主要是找到 配置文件 目录，如果没有指定则使用默认，主要是提供给第 2 步（加载配置）用    final String configurationDirectory = getConfigurationDirectoryFromEnv();    // 2. load the global configuration    // 根据第一步的配置目录加载全局配置    final Configuration configuration =            GlobalConfiguration.loadConfiguration(configurationDirectory);    // 3. load the custom command lines    // 加载自定义的命令    final List&lt;CustomCommandLine&gt; customCommandLines =            loadCustomCommandLines(configuration, configurationDirectory);    int retCode = 31;    try {        final CliFrontend cli = new CliFrontend(configuration, customCommandLines);        SecurityUtils.install(new SecurityConfiguration(cli.configuration));        // 解析命令行并开始请求        // 判断是否有 hadoop 权限安全控制走对应的 doas(), 具体执行逻辑为 cli.parseAndRun(args) 解析对应的用户参数        retCode = SecurityUtils.getInstalledContext().runSecured(() -&gt; cli.parseAndRun(args));    } catch (Throwable t) {        final Throwable strippedThrowable =                ExceptionUtils.stripException(t, UndeclaredThrowableException.class);        LOG.error("Fatal error while running command line interface.", strippedThrowable);        strippedThrowable.printStackTrace();    } finally {        System.exit(retCode);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着 <code>cli.parseAndRun(args)</code> 继续看逻辑</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java"> /** * Parses the command line arguments and starts the requested action. * * @param args command line arguments of the client. * @return The return code of the program */public int parseAndRun(String[] args) { // args 为脚本启动后面的参数    // check for action    if (args.length &lt; 1) {        CliFrontendParser.printHelp(customCommandLines);        System.out.println("Please specify an action.");        return 1;    }    // get action    // 这里的 action 是 run    String action = args[0];    // remove action from parameters    // 去除 run 之后剩下的参数    final String[] params = Arrays.copyOfRange(args, 1, args.length);    try {        // do action        // 判断 action        switch (action) {            case ACTION_RUN:   // private static final String ACTION_RUN = "run";  // 常量 run                run(params);   // 所以直接执行 run(params)                 return 0;            case ACTION_RUN_APPLICATION:                runApplication(params);                return 0;            case ACTION_LIST:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着<code>run(params);</code> 继续看逻辑</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">protected void run(String[] args) throws Exception {    LOG.info("Running 'run' command.");    // 这个 CliFrontendParser 包含所有命令的 option，如下图 flink -h 显示的所有选项    final Options commandOptions = CliFrontendParser.getRunCommandOptions();        // 解析对应的参数    final CommandLine commandLine = getCommandLine(commandOptions, args, true);    // evaluate help flag    // 判断为 help 的时候 打印出所有的 run 所需的 options    if (commandLine.hasOption(HELP_OPTION.getOpt())) {        CliFrontendParser.printHelpForRun(customCommandLines);        return;    }    // 先判空，然后获取所有 active 命令    final CustomCommandLine activeCommandLine =            validateAndGetActiveCommandLine(checkNotNull(commandLine));    final ProgramOptions programOptions = ProgramOptions.create(commandLine);    // 获取 job jars 和依赖的 libraries    final List&lt;URL&gt; jobJars = getJobJarAndDependencies(programOptions);        // 经过上述几个步骤 拿到最终生效的配置    final Configuration effectiveConfiguration =            getEffectiveConfiguration(activeCommandLine, commandLine, programOptions, jobJars);    LOG.debug("Effective executor configuration: {}", effectiveConfiguration);         // 根据上面得到的配置，先打包 program，然后 execute    try (PackagedProgram program = getPackagedProgram(programOptions, effectiveConfiguration)) {        executeProgram(effectiveConfiguration, program);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着 <code>getPackagedProgram(programOptions, effectiveConfiguration)</code> 继续看逻辑</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">//这个函数里基本逻辑就下面这一句program = buildProgram(programOptions, effectiveConfiguration);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>接着 <code>buildProgram(programOptions, effectiveConfiguration)</code>继续看逻辑</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">/** * Creates a Packaged program from the given command line options and the * effectiveConfiguration. * * @return A PackagedProgram (upon success) 返回值 */PackagedProgram buildProgram(final ProgramOptions runOptions, final Configuration configuration)        throws FileNotFoundException, ProgramInvocationException, CliArgsException {    runOptions.validate();        // 传入程序内部的参数    String[] programArgs = runOptions.getProgramArgs();    // jar 路径    String jarFilePath = runOptions.getJarFilePath();    List&lt;URL&gt; classpaths = runOptions.getClasspaths();    // Get assembler class    // 入口类名    String entryPointClass = runOptions.getEntryPointClassName();    File jarFile = jarFilePath != null ? getJarFile(jarFilePath) : null;    return PackagedProgram.newBuilder()            .setJarFile(jarFile)            .setUserClassPaths(classpaths)            .setEntryPointClassName(entryPointClass)            .setConfiguration(configuration)            .setSavepointRestoreSettings(runOptions.getSavepointRestoreSettings())            .setArguments(programArgs)            .build();  // build 触发构建, 这里可以看出 PackagedProgram 用到了 Builder Pattern（建造者模式）}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着 <code>.build();</code> 继续看逻辑</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public PackagedProgram build() throws ProgramInvocationException {    if (jarFile == null &amp;&amp; entryPointClassName == null) {        throw new IllegalArgumentException(                "The jarFile and entryPointClassName can not be null at the same time.");    }    // 这里会返回一个 PackagedProgram    // 这里其实就是拿到了 入口运行类 的全额限定名，然后通过类加载器加载运行主类    return new PackagedProgram(            jarFile,            userClassPaths,            entryPointClassName,            configuration,            savepointRestoreSettings,            args);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中，加载运行主类代码在构造函数里</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">// load the entry point classthis.mainClass =        loadMainClass(                // if no entryPointClassName name was given, we try and look one up through                // the manifest                entryPointClassName != null                        ? entryPointClassName                        : getEntryPointClassNameFromJar(this.jarFile),                userCodeClassLoader);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>loadMainClass()</code> 函数代码中可以看到</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">ClassLoader contextCl = null;try {    contextCl = Thread.currentThread().getContextClassLoader();    Thread.currentThread().setContextClassLoader(cl);    // 通过反射加载上面的 mainClass    return Class.forName(className, false, cl);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>得到了 PackagedProgram ，下面就是 <code>executeProgram(effectiveConfiguration, program);</code></p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">// --------------------------------------------------------------------------------------------//  Interaction with programs and JobManager// 注释这里已经写了，用户程序和 jm 的交互// --------------------------------------------------------------------------------------------protected void executeProgram(final Configuration configuration, final PackagedProgram program)        throws ProgramInvocationException {    ClientUtils.executeProgram(            new DefaultExecutorServiceLoader(), configuration, program, false, false);            // new DefaultExecutorServiceLoader() 这就是 Executor 装载器， 用于执行后续的 program}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建和初始化<code>ExecutorServiceLoader</code>接口的实现类<code>DefaultExecutorServiceLoader</code>，加载客户端代码的<code>PipelineExecutor</code>。<code>PipelineExecutor</code>是客户端专门用于执行应用程序代码的执行器，不同类型的集群对应不同的<code>PipelineExecutor</code>实现类。</p><p>接下来进入 <code>executeProgram()</code> 继续看逻辑</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">//通过ContextEnvironmentFactory创建和初始化ContextEnvironment。//ContextEnvironment是一种特殊的ExecutionEnvironment，仅在Flink Client中创建，专门用于通过命令行的方式提交作业。ContextEnvironment.setAsContext(    executorServiceLoader,    configuration,    userCodeClassLoader,    enforceSingleJobExecution,    suppressSysout);StreamContextEnvironment.setAsContext(    executorServiceLoader,    configuration,    userCodeClassLoader,    enforceSingleJobExecution,    suppressSysout);try {    program.invokeInteractiveModeForExecution();} finally {    ContextEnvironment.unsetAsContext();    StreamContextEnvironment.unsetAsContext();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在ContextEnvironment 和 StreamContextEnvironment 初始化完成后，ClientUtils会调用PackagedProgram.invokeInteractiveModeForExecution()方法。</p><p>进入<code>invokeInteractiveModeForExecution()</code>继续看逻辑</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">callMainMethod(mainClass, args);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个函数做了一系列校验后，</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">mainMethod.invoke(null, (Object) args);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>通过反射的方式执行用户程序 (jar) 中的 <code>main()</code> 方法。</p><h3 id="什么时候转换为-JobGraph-？"><a href="#什么时候转换为-JobGraph-？" class="headerlink" title="什么时候转换为 JobGraph ？"></a>什么时候转换为 JobGraph ？</h3><p>其实上述流程执行 <code>main()</code> 之前，<code>environment</code> 就已经准备好了，当执行用户程序的 <code>execute("Job name");</code>就会触发图的转换，最终提交给 JobManager，后面会单独一篇文章来写。</p>]]></content>
      
      
      <categories>
          
          <category> Flink 源码篇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> 源码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/08/13/she-ji-mo-shi/"/>
      <url>/2021/08/13/she-ji-mo-shi/</url>
      
        <content type="html"><![CDATA[<h4 id="1-设计原则"><a href="#1-设计原则" class="headerlink" title="1. 设计原则"></a>1. 设计原则</h4><h5 id="1-1-单一职责原则（SRP）"><a href="#1-1-单一职责原则（SRP）" class="headerlink" title="1.1 单一职责原则（SRP）"></a>1.1 单一职责原则（SRP）</h5><blockquote><p>A class or module should have a single responsibility。</p></blockquote><p>不要设计大而全的类，要设计粒度小、功能单一的类。</p><ul><li><p><strong>如何判断类的职责是否足够单一？</strong></p><p>不同的应用场景、不同阶段的需求背景、不同的业务层面，对同一个类的职责是否单一，可能会有不同的判定结果。实际上，一些侧面的判断指标更具有指导意义和可执行性，比如，出现下面这些情况就有可能说明这类的设计不满足单一职责原则：</p><ul><li>类中的代码行数、函数或者属性过多；</li><li>类依赖的其他类过多，或者依赖类的其他类过多；</li><li>私有方法过多；</li><li>比较难给类起一个合适的名字；</li><li>类中大量的方法都是集中操作类中的某几个属性。</li></ul></li><li><p><strong>类的职责是否设计得越单一越好？</strong></p><ul><li>单一职责原则通过避免设计大而全的类，避免将不相关的功能耦合在一起，来提高类的内聚性。同时，类职责单一，类依赖的和被依赖的其他类也会变少，减少了代码的耦合性，以此来实现代码的高内聚、低耦合。但是，如果拆分得过细，实际上会适得其反，反倒会降低内聚性，也会影响代码的可维护性。</li></ul></li></ul><p><strong>场景题</strong></p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">UserInfo</span> <span class="token punctuation">{</span>  <span class="token keyword">private</span> <span class="token keyword">long</span> userId<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> username<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> email<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> telephone<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token keyword">long</span> createTime<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token keyword">long</span> lastLoginTime<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> avatarUrl<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> provinceOfAddress<span class="token punctuation">;</span> <span class="token comment">// 省</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> cityOfAddress<span class="token punctuation">;</span> <span class="token comment">// 市</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> regionOfAddress<span class="token punctuation">;</span> <span class="token comment">// 区 </span>  <span class="token keyword">private</span> <span class="token class-name">String</span> detailedAddress<span class="token punctuation">;</span> <span class="token comment">// 详细地址</span>  <span class="token comment">// ...省略其他属性和方法...</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>UserInfo 类是否满足单一职责原则？</p><p>实际上，要从中做出选择，我们不能脱离具体的应用场景。如果在这个社交产品中，用户的地址信息跟其他信息一样，只是单纯地用来展示，那 UserInfo 现在的设计就是合理的。但是，如果这个社交产品发展得比较好，之后又在产品中添加了电商的模块，用户的地址信息还会用在电商物流中，那我们最好将地址信息从 UserInfo 中拆分出来，独立成用户物流信息（或者叫地址信息、收货信息等）。</p><p>我们再进一步延伸一下。如果做这个社交产品的公司发展得越来越好，公司内部又开发出了很多其他产品（可以理解为其他 App）。公司希望支持统一账号系统，也就是用户一个账号可以在公司内部的所有产品中登录。这个时候，我们就需要继续对 UserInfo 进行拆分，将跟身份认证相关的信息（比如，email、telephone 等）抽取成独立的类。</p><p><strong>总结：不同的应用场景、不同阶段的需求背景下，对同一个类的职责是否单一的判定，可能都是不一样的</strong>。</p><h5 id="1-2-开闭原则（OCP）"><a href="#1-2-开闭原则（OCP）" class="headerlink" title="1.2 开闭原则（OCP）"></a>1.2 开闭原则（OCP）</h5><blockquote><p>Software entities (modules, classes, functions, etc.) should be open for extension , but closed for modification。</p></blockquote><p>软件实体（模块、类、方法等）应该“对扩展开放、对修改关闭”。</p><ul><li><p><strong>如何理解“对扩展开放、对修改关闭”？</strong></p><p>添加一个新的功能，应该是通过在已有代码基础上扩展代码（新增模块、类、方法、属性等），而非修改已有代码（修改模块、类、方法、属性等）的方式来完成。关于定义，我们有两点要注意。第一点是，开闭原则并不是说完全杜绝修改，而是以最小的修改代码的代价来完成新功能的开发。第二点是，同样的代码改动，在粗代码粒度下，可能被认定为“修改”；在细代码粒度下，可能又被认定为“扩展”。</p></li><li><p><strong>如何做到“对扩展开放、修改关闭”？</strong></p><p>我们要时刻具备扩展意识、抽象意识、封装意识。在写代码的时候，我们要多花点时间思考一下，这段代码未来可能有哪些需求变更，如何设计代码结构，事先留好扩展点，以便在未来需求变更的时候，在不改动代码整体结构、做到最小代码改动的情况下，将新的代码灵活地插入到扩展点上。</p><p>很多设计原则、设计思想、设计模式，都是以提高代码的扩展性为最终目的的。特别是 23 种经典设计模式，大部分都是为了解决代码的扩展性问题而总结出来的，都是以开闭原则为指导原则的。最常用来提高代码扩展性的方法有：多态、依赖注入、基于接口而非实现编程，以及大部分的设计模式（比如，装饰、策略、模板、职责链、状态）。</p></li></ul><h5 id="1-3-里式替换（LSP）"><a href="#1-3-里式替换（LSP）" class="headerlink" title="1.3 里式替换（LSP）"></a>1.3 里式替换（LSP）</h5><blockquote><p>Functions that use pointers of references to base classes must be able to use objects of derived classes without knowing it。</p></blockquote><p>子类对象（object of subtype/derived class）能够替换程序（program）中父类对象（object of base/parent class）出现的任何地方，并且保证原来程序的逻辑行为（behavior）不变及正确性不被破坏。</p><p>里式替换原则是用来指导，继承关系中子类该如何设计的一个原则。理解里式替换原则，最核心的就是理解“design by contract，按照协议来设计”这几个字。父类定义了函数的“约定”（或者叫协议），那子类可以改变函数的内部实现逻辑，但不能改变函数原有的“约定”。这里的约定包括：函数声明要实现的功能；对输入、输出、异常的约定；甚至包括注释中所罗列的任何特殊说明。</p><p>理解这个原则，我们还要弄明白里式替换原则跟多态的区别。虽然从定义描述和代码实现上来看，多态和里式替换有点类似，但它们关注的角度是不一样的。多态是面向对象编程的一大特性，也是面向对象编程语言的一种语法。它是一种代码实现的思路。而里式替换是一种设计原则，用来指导继承关系中子类该如何设计，子类的设计要保证在替换父类的时候，不改变原有程序的逻辑及不破坏原有程序的正确性。</p><p>里氏替换就是子类完美继承父类的设计初衷，并做了增强。</p><h5 id="1-4-接口隔离原则（ISP）"><a href="#1-4-接口隔离原则（ISP）" class="headerlink" title="1.4 接口隔离原则（ISP）"></a>1.4 接口隔离原则（ISP）</h5><blockquote><p>Clients should not be forced to depend upon interfaces that they do not use。</p></blockquote><p>客户端不应该被强迫依赖它不需要的接口。其中的“客户端”，可以理解为接口的调用者或者使用者。</p><ul><li><p>如何理解“接口隔离原则”？</p><p>理解“接口隔离原则”的重点是理解其中的“接口”二字。这里有三种不同的理解。</p><p>如果把“接口”理解为一组接口集合，可以是某个微服务的接口，也可以是某个类库的接口等。如果部分接口只被部分调用者使用，我们就需要将这部分接口隔离出来，单独给这部分调用者使用，而不强迫其他调用者也依赖这部分不会被用到的接口。</p><p>如果把“接口”理解为单个 API 接口或函数，<strong>部分调用者只需要函数中的部分功能</strong>，那我们就需要把函数拆分成粒度更细的多个函数，让调用者只依赖它需要的那个细粒度函数。</p><p>如果把“接口”理解为 OOP 中的接口，也可以理解为面向对象编程语言中的接口语法。那接口的设计要尽量单一，不要让接口的实现类和调用者，依赖不需要的接口函数。</p></li></ul><h5 id="1-5-依赖反转原则（DIP）"><a href="#1-5-依赖反转原则（DIP）" class="headerlink" title="1.5 依赖反转原则（DIP）"></a>1.5 依赖反转原则（DIP）</h5><blockquote><p>High-level modules shouldn’t depend on low-level modules. Both modules should depend on abstractions. In addition, abstractions shouldn’t depend on details. Details depend on abstractions.</p></blockquote><p>高层模块（high-level modules）不要依赖低层模块（low-level）。高层模块和低层模块应该通过抽象（abstractions）来互相依赖。除此之外，抽象（abstractions）不要依赖具体实现细节（details），具体实现细节（details）依赖抽象（abstractions）。</p><p>所谓高层模块和低层模块的划分，简单来说就是，在调用链上，调用者属于高层，被调用者属于低层。</p><p><strong>场景</strong></p><p>Tomcat 是运行 Java Web 应用程序的容器。我们编写的 Web 应用程序代码只需要部署在 Tomcat 容器下，便可以被 Tomcat 容器调用执行。按照之前的划分原则，Tomcat 就是高层模块，我们编写的 Web 应用程序代码就是低层模块。Tomcat 和应用程序代码之间并没有直接的依赖关系，两者都依赖同一个“抽象”，也就是 Servlet 规范。Servlet 规范不依赖具体的 Tomcat 容器和应用程序的实现细节，而 Tomcat 容器和应用程序依赖 Servlet 规范。</p><ul><li><p>控制反转</p><p>实际上，控制反转是一个比较笼统的设计思想，并不是一种具体的实现方法，一般用来指导框架层面的设计。这里所说的“控制”指的是对程序执行流程的控制，而“反转”指的是在没有使用框架之前，程序员自己控制整个程序的执行。在使用框架之后，整个程序的执行流程通过框架来控制。流程的控制权从程序员“反转”给了框架。</p></li><li><p>依赖注入</p><p>依赖注入和控制反转恰恰相反，它是一种具体的编码技巧。我们不通过 new 的方式在类内部创建依赖类的对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类来使用。</p></li><li><p>依赖注入框架</p><p>我们通过依赖注入框架提供的扩展点，简单配置一下所有需要的类及其类与类之间依赖关系，就可以实现由框架来自动创建对象、管理对象的生命周期、依赖注入等原本需要程序员来做的事情。</p></li><li><p>依赖反转原则</p><p>依赖反转原则也叫作依赖倒置原则。这条原则跟控制反转有点类似，主要用来指导框架层面的设计。高层模块不依赖低层模块，它们共同依赖同一个抽象。抽象不要依赖具体实现细节，具体实现细节依赖抽象。</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基于 Apache Spark SQL 的交互式查询在 Pinterest 的实践</title>
      <link href="/2021/08/12/ji-yu-apache-spark-sql-de-jiao-hu-shi-cha-xun-zai-pinterest-de-shi-jian/"/>
      <url>/2021/08/12/ji-yu-apache-spark-sql-de-jiao-hu-shi-cha-xun-zai-pinterest-de-shi-jian/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文作者：<br>Sanchay Javeria | Software Engineer, Big Data Query Platform, Data Engineering<br>Ashish Singh | Technical Lead, Big Data Query Platform, Data Engineering</p></blockquote><h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><ul><li>为什么要基于 Spark SQL 做交互式查询，Presto 不是更好的选择吗？</li></ul><p>查询平台的用户都是与业务有关的，数仓、分析师、业务开发、运营等等，面向的对象不同，使用的引擎也就不同，Presto 的交互式查询体验好，都可以满足，但是日常工作中，数仓和分析师同学都需要写 Spark SQL 脚本，那么就需要有个地方来验证脚本，基于 Spark SQL 的交互式查询就是用来满足这个需求的理想方案。</p><p>Presto 查询是有限规模的快速查询，而 Spark SQL 是用来支持全规模的查询。</p><p>—下面是正文—</p><hr><p>为了实现我们的使命，即通过我们的视觉发现引擎为每个人带来灵感，Pinterest 严重依赖于制定数据驱动的决策，以改善超过 4.75 亿月活跃用户的 Pinner 体验。可靠、快速和可扩展的交互式查询对于使这些数据驱动的决策至关重要。过去，我们发布了 <a href="https://medium.com/pinterest-engineering/presto-at-pinterest-a8bda7515e52" title="Presto">Pinterest 的 Presto</a> 如何提供此功能。在这里，我们将分享我们如何构建一个可扩展、可靠且高效的交互式查询平台，该平台每天使用 Apache Spark SQL 处理数百 PB 的数据。通过对各种架构选择、过程中的挑战以及我们对这些挑战的解决方案的详细讨论，我们分享了如何使用 Spark SQL 成功地进行交互式查询。</p><h4 id="计划-Scheduled-查询与交互式-Interactive-查询"><a href="#计划-Scheduled-查询与交互式-Interactive-查询" class="headerlink" title="计划 (Scheduled) 查询与交互式 (Interactive) 查询"></a>计划 (Scheduled) 查询与交互式 (Interactive) 查询</h4><p>查询是用户从 Pinterest 数据中获得理解的最流行方式。此类分析的应用存在于所有业务/工程功能中，例如机器学习、广告、搜索、家庭供稿推荐、信任与安全等。提交这些查询主要有两种方式：计划和交互式。</p><ol><li>计划查询是按预定义节奏运行的查询，这些查询通常具有严格的服务级别目标 (SLO)。</li><li>交互式查询是在需要时执行的查询，通常不会以预定义的节奏重复。与预定查询不同，用户等待交互式查询完成并且不知道可能导致查询失败的潜在问题。这些特点使得交互式查询平台的需求不同于预定查询平台。</li></ol><p>以下部分，我们将深入探讨如何在 Pinterest 上使用 Spark SQL 扩展交互式查询。我们首先讨论如何在 Pinterest 上使用 Spark SQL 以及特定于使用 Spark SQL 进行交互式查询的挑战。我们通过介绍架构进行跟进，并讨论我们如何解决我们在此过程中面临的挑战。</p><h4 id="使用-Spark-SQL-进行交互式查询"><a href="#使用-Spark-SQL-进行交互式查询" class="headerlink" title="使用 Spark SQL 进行交互式查询"></a>使用 Spark SQL 进行交互式查询</h4><p>我们支持 Hive、Presto 和 Spark SQL 来查询数据。但是，我们正在弃用 Hive 以支持 Spark SQL，这给我们留下了两个主要的查询引擎（即 Presto 和 Spark SQL）。Presto 用于快速交互查询。Spark SQL 用于所有计划查询（在 Hive 弃用完成后不久）和对大型数据集的交互式查询。以下是我们在从 Hive 迁移到 Spark SQL 时考虑支持使用 Spark SQL 进行交互式查询的各种方法。</p><h5 id="Apache-Spark-的-Thrift-JDBC-ODBC-服务器"><a href="#Apache-Spark-的-Thrift-JDBC-ODBC-服务器" class="headerlink" title="Apache Spark 的 Thrift JDBC/ODBC 服务器"></a>Apache Spark 的 Thrift JDBC/ODBC 服务器</h5><p>Apache Spark 的 Thrift JDBC/ODBC Server (STS) 类似于 HiveServer2，允许客户端通过 JDBC/ODBC 协议执行 Spark SQL 查询。JDBC/ODBC 协议是各种客户端提交查询的最流行方式之一。使用 STS 将允许现有的 JDBC/ODBC 协议支持工具与 Spark SQL 无缝协作。然而，这种方法并没有在提交到同一个 thrift 服务器的查询之间提供适当的隔离。</p><p>单个查询的问题可能会影响在同一节点服务器上运行的所有其他查询。过去使用 Hiveserver2 进行交互式查询时，我们看到了几个问题，即错误的查询导致整个服务器宕机，导致所有并发运行的查询终止/失败。大多数情况下，它要么是由于在本地模式下运行的单个查询在查询优化中占用过多内存，要么是由于查询加载了本地 jar 导致服务器内核崩溃。根据我们的经验，决定不选择这种方法。</p><h5 id="Spark-SQL-查询作为-Apache-YARN-上的-shell-命令应用程序"><a href="#Spark-SQL-查询作为-Apache-YARN-上的-shell-命令应用程序" class="headerlink" title="Spark SQL 查询作为 Apache YARN 上的 shell 命令应用程序"></a>Spark SQL 查询作为 Apache YARN 上的 shell 命令应用程序</h5><p>运行 Spark SQL 查询的另一种常见机制是通过 spark-sql 命令行界面 (CLI)。但是，CLI 方法不适用于交互式应用程序，并且不能提供最佳用户体验。</p><p>可以在我们的 YARN 集群上从各种客户端构建一个服务，该服务将 spark-sql CLI 作为 shell 命令应用程序启动。但是，这会导致在 YARN 集群上等待容器分配，然后为每个查询启动一个 Spark 会话的前期成本。此过程最多可能需要几分钟，具体取决于集群上的资源可用性。</p><p>这种方法会导致交互式查询的用户体验不佳，例如，因为用户需要等待几分钟才能找到语法问题。此外，这种方法使得检索结果、提供语句级进度更新或在发生故障时从驱动程序日志中获取异常堆栈跟踪变得困难。这些是我们对出色的交互式查询体验的一些要求。</p><h5 id="Apache-Livy-与批处理会话"><a href="#Apache-Livy-与批处理会话" class="headerlink" title="Apache Livy 与批处理会话"></a>Apache Livy 与批处理会话</h5><p>Apache Livy 是一种服务，可通过 RESTful 接口与 Spark 集群进行交互。使用 Livy，我们可以轻松地将 Spark SQL 查询提交到我们的 YARN 集群，并通过简单的 REST 调用管理 Spark 上下文。这是对我们复杂的 Spark 基础架构的理想抽象，并允许与面向用户的客户端直接集成。</p><p>Livy 提供两种作业提交选项：批处理和交互式。批处理模式类似于用于提交批处理应用程序的 spark-submit。在批处理模式下，查询的所有语句一起提交执行。这使得我们为交互式查询设想的一些可用性功能变得困难，例如：根据语句对在哪里运行查询做出不同的选择，支持使用 SQL 语句更改 spark 会话的功能，以及创建可重用的用户会话/缓存. 我们将在本文后面详细讨论这些功能。</p><h5 id="Apache-Livy-与交互式会话"><a href="#Apache-Livy-与交互式会话" class="headerlink" title="Apache Livy 与交互式会话"></a>Apache Livy 与交互式会话</h5><p>与 Apache Livy 的批处理会话不同，交互式会话使我们能够启动会话，将查询和/或语句作为单独的请求提交，并在完成时明确结束会话。</p><p>此外，Livy 通过会话恢复和故障隔离提供多租户、高可用性，这些都是我们的首要架构优先事项。这帮助我们选择 Livy 作为 Pinterest 交互式 Spark SQL 查询的理想解决方案。</p><h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p>下面的图 1 概述了 Spark SQL 的查询执行架构和交互式查询用例的请求流。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628739720821-architecture.png" alt="图 1 在 Pinterest 上使用 Spark SQL 进行预定和交互式查询的请求流"></p><p>该图提出的一个明显问题是为什么我们需要分别处理 DDL 和 DML 查询。我们将在后面讨论，同时讨论我们在使用 Spark SQL 进行交互式查询时所面临的挑战以及我们如何解决这些挑战。下面针对交互式 DML 和 DDL 查询详细说明图 1 中的控制流。</p><h5 id="交互式-DML-查询"><a href="#交互式-DML-查询" class="headerlink" title="交互式 DML 查询"></a>交互式 DML 查询</h5><ol><li>Querybook 和 Jupyter 等客户端向 Livy 提交交互式 DML 查询；</li><li>Livy 从 YARN 资源管理器 (RM) 请求容器以运行远程 Spark 上下文 (RSC) 客户端；</li><li>RM 分配一个容器，在其中启动 RSC 客户端。此 RSC 客户端然后启动 RSC 驱动程序；</li><li>Livy 通过与运行驱动程序的 RSC 客户端通信来跟踪查询进度；</li><li>Spark SQL 驱动程序根据需要从 Hive Metastore Service (HMS) 获取查询计划所需的表元数据；</li><li>根据资源需求，驱动程序向 RM 请求容器以启动执行程序；</li><li>Spark SQL 驱动程序在执行器之间分配任务和协调工作，直到为用户查询完成所有 Spark 作业。</li></ol><h5 id="交互式-DDL-查询"><a href="#交互式-DDL-查询" class="headerlink" title="交互式 DDL 查询"></a>交互式 DDL 查询</h5><ol><li>Client 向 Livy 提交交互式 DDL 查询。</li><li>Livy 从本地会话池中获取 Spark 会话（详细信息将在后面的部分中讨论），并正确更新当前请求用户的用户凭据。</li><li>本地 Spark SQL 驱动从 HMS 获取用于查询规划的表元数据，并根据需要执行 DDL 操作。</li></ol><h4 id="挑战和我们的解决方案"><a href="#挑战和我们的解决方案" class="headerlink" title="挑战和我们的解决方案"></a>挑战和我们的解决方案</h4><p>本节讨论我们必须解决的各种挑战，以便在 Pinterest 上使用 Spark SQL 进行交互式查询。</p><h5 id="无缝查询提交"><a href="#无缝查询提交" class="headerlink" title="无缝查询提交"></a>无缝查询提交</h5><p>虽然 Livy 提供了一种可靠的解决方案来将查询作为 Spark 作业提交，但我们需要用户使用标准接口从任何客户端提交查询，该接口可用作插入式依赖项以轻松与 Livy 通信。</p><p>我们在 Livy 之上构建了一个通用的 DB-API 兼容 Python 客户端，称为 BigPy，多个查询客户端使用它来提交查询。在 BigPy 中，我们提供了一个接口来实现以下功能：</p><ul><li>状态轮询：它监视 Livy 会话的状态并向客户端报告应用程序是否成功、失败或当前正在运行。此外，我们还报告了 Spark 应用程序的完成百分比；</li><li>跟踪链接：返回所有跟踪链接，用于监控 Spark 应用程序的状态，包括指向 Spark UI、驱动程序日志和 Dr.Elephant 的链接，用于监控 Spark 应用程序的性能和调优；</li><li>结果检索：它提供了从 AWS S3 等对象存储中以分页方式检索查询结果的能力；</li><li>异常检索：Spark 驱动程序和执行程序日志通常很嘈杂，查找查询失败的原因可能很麻烦。BigPy 返回异常，其堆栈跟踪直接到客户端，以获得更轻松的调试体验。</li></ul><p>BigPy 启用了跨多个不同系统与 Livy 交互的模块化方式，提供了关注点与客户端代码的明确分离。</p><h5 id="快速元数据查询"><a href="#快速元数据查询" class="headerlink" title="快速元数据查询"></a>快速元数据查询</h5><p>spark-shell 程序以 cluster mode 向 RM 发送 YARN 应用程序请求。RM 启动 Application Master (AM)，然后启动驱动程序。驱动程序进一步向 RM 请求用于启动执行程序的更多容器。我们发现，这个资源分配过程可能需要几分钟才能开始处理每个查询，这会显着增加数据定义语言 (DDL)/仅元数据查询的延迟，这些查询通常是低延迟的元数据操作。</p><p>DDL 查询在驱动程序上执行，不需要额外的执行程序或与 DML 查询相同数量的隔离。为了缓解 YARN 集群上容器分配的冗余延迟和 SparkSession 启动时间的问题，我们在 Apache Livy 中实现了一个本地会话池，它维护了一个以本地模式运行的 Spark 会话池。</p><p>这个问题有两个部分：1) 将查询识别为 DDL 语句，以及 2) 实现 Spark 应用程序的缓存池来处理这些查询。我们利用 SparkSqlParser 为用户查询获取逻辑计划以识别 DDL 查询。由于这个逻辑计划只是一个继承自 TreeNode 类的逻辑运算符树，我们可以轻松地遍历这棵树并根据一组 DDL 执行命令检查每个节点的类。如果逻辑计划的所有节点都与 DDL 命令匹配，我们将查询标识为 DDL。在实践中，它看起来像这样：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628746567392-image.png"></p><p>一旦我们知道查询是 DDL 语句，我们就将其路由到缓存的 Spark 应用程序之一。我们在 Livy 中构建了这个缓存的应用程序池，由本地运行的 Spark 驱动程序池表示。它旨在完全自力更生，具有以下功能：</p><ul><li>过时应用程序的自动垃圾收集并启动新应用程序</li><li>一个守护线程监控池的健康状况并将查询路由到下一个可用的应用程序</li><li>以可配置的节奏重新启动应用程序，以确保它获取最新的资源（例如架构 jar）以确保数据新鲜度</li><li>在开始时异步启动轻量级元数据操作以初始化 SparkContext 并建立与 Metastore 的实时连接以加快后续操作</li></ul><p>通过这种设计，我们将查询延迟从 70 秒减少到平均 10 秒（约 6.3 倍的改进）。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628746645987-image.png" alt="图 2：在 local 与 cluster mode 下运行的 DDL 查询的时间比较"></p><h5 id="快速失败：更快的语法检查"><a href="#快速失败：更快的语法检查" class="headerlink" title="快速失败：更快的语法检查"></a>快速失败：更快的语法检查</h5><p>在 cluster mode 下运行每个查询的另一个缺点是语法检查将至多花费在最坏情况下启动应用程序所需的时间。在临时环境中，用户通常希望更早地出现语法问题，等待几分钟才报告语法问题会带来令人失望的体验。我们通过使用 SparkSqlParser 改进了这一点，并在启动 YARN 应用程序之前获取查询的逻辑计划。如果查询包含语法错误，解析器将在生成逻辑计划时抛出“ParseException”，并方便地返回行号和列号，我们将其报告给客户端。通过这种方式，我们将整体语法检查延迟从几分钟减少到不到两秒（改进了 30 倍以上）。</p><h5 id="错误处理建议"><a href="#错误处理建议" class="headerlink" title="错误处理建议"></a>错误处理建议</h5><p>查询失败在临时环境中是隐含的。但是，修复这些故障通常是一个艰巨的循环，需要浏览驱动程序日志、通过自我诊断或寻求外部帮助找到解决方案，然后重试查询。为了简化此过程，我们提供了一些常见问题的自动故障排除信息，这些问题乍一看很难修复。此解决方案有四个部分：</p><ol><li>根据上次查询的执行状态判断 YARN 申请失败</li></ol><p>cluster mode 下 Livy Interactive Sessions 的一个问题是它们始终向 YARN AM 报告“成功”状态。发生这种情况是因为 Livy 提交给 SparkLauncher 的远程驱动程序启动了一个 Spark 上下文，在该上下文中运行一些查询，然后关闭该上下文。无论查询运行的状态如何，报告的最终状态始终是 SparkContext 是否能够成功关闭。这会误导用户和平台所有者。为了缓解这个问题，我们在单个交互式会话中跟踪最终查询运行的状态，并在查询失败时在远程驱动程序中抛出运行时异常。这有助于将状态正确报告回 AM 并使用故障原因（如果有）填充 YARN 诊断。</p><ol start="2"><li>识别用户查询中的常见错误</li></ol><p>一旦我们使用查询的失败原因正确填充 YARN 诊断，我们就会利用添加到 YARN 集群的额外日志记录来方便地跟踪 Spark SQL 表中遇到的错误。然后，我们查看了失败堆栈跟踪的历史记录，并使用正则表达式对它们进行了分类。根据频率，我们获得了一个 top-n 错误列表。<br>我们利用 Dr.Elephant 跟踪 Spark 应用程序启发式和指标，并添加了错误分类机制，该机制查看应用程序的 YARN 诊断信息并基于正则表达式引擎对其进行分类。使用上述正则表达式，我们将通过 REST API 公开的常见错误的故障排除信息添加到 Dr.Elephant Web UI 和其他外部客户端（如 Querybook）。</p><ol start="3"><li>Livy 集成 Dr.Elephant</li></ol><p>我们在 Livy 中为每个启动的 Spark 应用程序集成了上面提到的 Dr.Elephant API。此端点在每次查询运行时返回给客户端，便于查看故障排除信息。</p><ol start="4"><li>客户端集成</li></ol><p>从 Livy 获取 Dr.Elephant 故障排除分析端点后，客户端从 API 中提取此信息并将其显示在查询日志中。这样，当我们看到查询失败时，我们可以提供常见错误的故障排除信息，帮助用户更快地诊断问题。</p><h5 id="资源利用可见性"><a href="#资源利用可见性" class="headerlink" title="资源利用可见性"></a>资源利用可见性</h5><p>查看我们的临时集群的历史内存消耗指标，我们注意到应用程序经常过度分配执行程序和驱动程序内存，导致不必要的资源浪费。另一方面，对于内存不足 (OOM) 的应用程序，我们的用户经常要求我们让他们更容易抢先捕获这些问题，以便更快地重新调整他们的查询。</p><p>为了解决这个问题，我们直接在客户端上显示实时内存消耗信息，所有执行程序使用不同的聚合，如最大、最小和平均内存。我们还标记消费不足和过度消费，并根据启发式提示用户采取行动。</p><p>我们使用 <a href="https://spark.apache.org/docs/latest/monitoring.html#metrics" title="Spark 指标库">Spark 指标库</a> 的自定义指标接收器为每个 Spark 应用程序收集实时内存消耗信息。然后我们在 BigPy 中使用这些指标并检查它们是否违反了任何资源阈值，以 UI 友好的降价表格式将信息返回给客户端。这种方法的一个例子可以在下面 GIF 的 Querybook 上看到：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628747458010-metrics.gif" alt="图 3：具有各种聚合的实时驱动程序/执行程序内存消耗信息"></p><h5 id="大结果处理和状态跟踪"><a href="#大结果处理和状态跟踪" class="headerlink" title="大结果处理和状态跟踪"></a>大结果处理和状态跟踪</h5><p>默认情况下，Livy 对查询结果集的限制为 1,000 行。增加这个限制并不理想，因为结果集存储在内存中，增加这个限制可能会导致像我们这样的内存限制环境中的大规模问题。为了解决这个问题，我们为每个查询的最终结果实施了 AWS S3 重定向。这样，大型结果集可以以多部分方式上传到 S3，而不会影响服务的整体性能。在客户端，我们稍后检索 REST 响应中返回的最终 S3 输出路径，并以分页方式从 S3 获取结果。这使得检索速度更快，而不会在列出路径对象时冒 S3 超时的风险。此重定向也可在查询级别进行配置，以便如果用户期望查询返回小于 1，我们还提供实时进度更新，这是通过对 Spark SQL 查询的已完成任务和活动任务总数与任务总数进行平均而获得的。可以在上面图 3 的 GIF 中看到预览。</p><h4 id="Livy-操作改进"><a href="#Livy-操作改进" class="headerlink" title="Livy 操作改进"></a>Livy 操作改进</h4><p>我们平均每天会看到大约 1,500 个临时 SparkSQL 查询，为了支持这种负载，我们的系统必须对我们的用户保持健康和可靠。我们对可靠性和稳定性进行了大量改进，使我们能够为 Livy 保持 99.5% 的正常运行时间 SLO。一些关键亮点：</p><ul><li>有效的 Livy 负载平衡</li></ul><p>按照设计，Livy 是一个有状态的 Web 服务。它将会话的状态存储在内存中，如查询运行、每个查询的状态、最终结果等。由于我们的客户端遵循 HTTP 轮询机制来获取这些属性，因此很难在上层添加经典/应用程序负载均衡器。为了解决这个问题，我们通过以循环方式将每个查询路由到最不忙的 Livy 实例，在应用程序级别实现了我们的负载平衡算法。此处，“繁忙度”由在特定 Livy 实例上运行的“活动”会话数定义。这种简单但有效的机制使我们能够在整个车队中更均匀地分配负载。</p><ul><li>指标和日志记录改进</li></ul><p>我们为 Livy 添加了事件监听器支持，其中事件被定义为任何 Livy 活动，包括会话创建和向会话提交语句。我们使用这些侦听器将 JSON 对象记录到本地磁盘以跟踪各种事件。这可以在出现问题时更快地进行调试和使用情况监控。</p><ul><li>指标</li></ul><p>我们还使用 Scalatra Metrics 来跟踪关键服务级别指标，例如健康检查、MAU、用户/查询的 DAU 计数、缓存会话命中率、查询成功率等。这些顶级指标对于跟踪我们集群中的整体临时活动非常重要。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>为了支持使用 SQL 分析和处理数百 PB 的数据，我们正在 Pinterest 上融合 Spark SQL 和 Presto。虽然 Presto 仍然是资源需求有限的快速交互式查询最受欢迎的查询引擎选择，但我们使用 Spark SQL 来支持所有规模的查询。交互式查询用例与计划查询有不同的要求。其中一些功能是无缝查询提交、快速元数据查询、快速语法检查以及更好的调试和调整支持。根据我们对交互式查询的需求以及可用开源解决方案提供的功能，我们决定使用 Apache Livy 构建 Spark SQL 交互式查询平台。但是，Livy 没有立即满足我们的要求，我们添加了各种功能来弥补这一差距。在这篇文章中，我们对我们的架构选择和增强功能进行了推理，以使 Pinterest 的交互式查询取得成功。我们计划将这些更改中的大部分回馈给开源社区。</p><blockquote><p>本文翻译自：<a href="https://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b">https://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大厂实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark SQL </tag>
            
            <tag> 交互式查询 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/06/28/yun-yuan-sheng-shu-ju-zhong-tai-du-shu-bi-ji/"/>
      <url>/2021/06/28/yun-yuan-sheng-shu-ju-zhong-tai-du-shu-bi-ji/</url>
      
        <content type="html"><![CDATA[<h3 id="《云原生数据中台》读书笔记"><a href="#《云原生数据中台》读书笔记" class="headerlink" title="《云原生数据中台》读书笔记"></a>《云原生数据中台》读书笔记</h3><p>大数据源于硅谷，国内数据中台的概念源于阿里巴巴，虽然硅谷没有“数据中台”的叫法，但硅谷的公司早已形成了中台的意识（来源于避免重复造轮子，快速迭代，数据驱动，业务驱动的工程师文化理念）。</p><h4 id="数据中台的思路"><a href="#数据中台的思路" class="headerlink" title="数据中台的思路"></a>数据中台的思路</h4><p>中台提供数据能力的共享和复用，前端业务部门可以快速获得全局的数据洞见及现成的数据工具，快速推出由数据支持的产品。</p><h4 id="数据中台要解决的问题"><a href="#数据中台要解决的问题" class="headerlink" title="数据中台要解决的问题"></a>数据中台要解决的问题</h4><ul><li>各个部门重复开发数据，浪费存储与计算资源；</li><li>数据标准不统一，数据使用成本高；</li><li>业务数据孤岛问题严重，数据利用率低。</li></ul><p>数据规范：连接生产数据的业务部门与消费数据的分析部门的桥梁。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>[Flink 实践篇-1] Flink 1.12.1 实现 WordCount 入门案例</title>
      <link href="/2021/06/26/flink-shi-jian-pian-1-flink-1-12-1-shi-xian-wordcount-ru-men-an-li/"/>
      <url>/2021/06/26/flink-shi-jian-pian-1-flink-1-12-1-shi-xian-wordcount-ru-men-an-li/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-26/1624694497803-flink-home-graphic.png" alt="Flink"></p><h4 id="什么是-WordCount-？"><a href="#什么是-WordCount-？" class="headerlink" title="什么是 WordCount ？"></a>什么是 WordCount ？</h4><p>wordcount 简单来讲就是单词计数，是一般大数据计算框架（Hadoop、Spark、Flink）的入门学习案例，相当于编程语言（Java、Python）中的 HelloWorld 案例，适合刚开始了解 Flink 作业提交流程的同学。</p><h4 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h4><ol><li>JDK 1.8 (必须)</li></ol><pre class="line-numbers language-linux" data-language="linux"><code class="language-linux">~  $ java -versionjava version "1.8.0_291"Java(TM) SE Runtime Environment (build 1.8.0_291-b10)Java HotSpot(TM) 64-Bit Server VM (build 25.291-b10, mixed mode)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>Flink 1.12.1（必须）</li></ol><ul><li>采用 tar 包安装的方式，下载地址如下：</li></ul><p><a href="https://archive.apache.org/dist/flink/flink-1.12.1/">https://archive.apache.org/dist/flink/flink-1.12.1/</a></p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623946032805-image.png" alt="下载 Flink 安装包"></p><ul><li>解压安装包</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev  $ <span class="token function">tar</span> -zxvf flink-1.12.1-bin-scala_2.11.tgz~/flink-dev  $ <span class="token function">ls</span> -ltotal <span class="token number">655424</span>drwxr-xr-x@ <span class="token number">13</span> it  staff        <span class="token number">416</span>  <span class="token number">1</span> <span class="token number">10</span> 08:46 flink-1.12.1-rw-r--r--@  <span class="token number">1</span> it  staff  <span class="token number">334271560</span>  <span class="token number">6</span> <span class="token number">18</span> 00:18 flink-1.12.1-bin-scala_2.11.tgz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>查看目录结构</li></ul><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623975272045-image.png" alt="Flink 目录结构"></p><ul><li><p>配置环境变量</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1  $ <span class="token builtin class-name">pwd</span>/xxx/flink-dev/flink-1.12.1~/flink-dev/flink-1.12.1  $ <span class="token function">vi</span> ~/.zshrc<span class="token comment"># 将下面两行添加到 上述文件中</span><span class="token comment"># 因为我这里用的是 zsh ，根据自己的需要选择文件</span><span class="token comment"># mac 默认是 .bash_profile </span><span class="token builtin class-name">export</span> <span class="token assign-left variable">FLINK_HOME</span><span class="token operator">=</span>/xxx/flink-dev/flink-1.12.1<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$FLINK_HOME</span>/bin<span class="token comment"># 保存之后执行以下命令</span><span class="token builtin class-name">source</span> ~/.zshrc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>验证版本</p></li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1  $ flink --versionVersion: <span class="token number">1.12</span>.1, Commit ID: dc404e2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如上显示，则表示安装成功。</p><h4 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h4><p>学过 一些 Java 的用户应该了解，启动集群脚本一般在 bin 目录下</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623976416741-image.png" alt="bin 目录下的脚本"></p><ul><li>执行启动集群命令<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1/bin  $ ./start-cluster.shStarting cluster.Starting standalonesession daemon on <span class="token function">host</span> MacBook-Pro.lan.Starting taskexecutor daemon on <span class="token function">host</span> MacBook-Pro.lan.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>– 访问 Flink webUI 界面 (localhost:8081)</p><p>如果输入地址后可以看到以下页面，表示集群启动成功。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623976909945-image.png" alt="输入地址"></p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623976953750-image.png" alt="web UI"></p><h4 id="编写-wordcount-项目"><a href="#编写-wordcount-项目" class="headerlink" title="编写 wordcount 项目"></a>编写 wordcount 项目</h4><p>Flink 本地启动了一个集群，接下来就是提交我们的任务，任务使用 Java 语言调用 Flink 的 api 来编写。</p><ul><li>创建项目<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624006853454-WeChat0292626db4546a6b740b2b0bdc12c3a3-min.png" alt="New Project"></li></ul><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624006989673-image.png"></p><ul><li>将以下配置复制到 pom.xml 文件</li></ul><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>project.build.sourceEncoding</span><span class="token punctuation">&gt;</span></span>UTF-8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>project.build.sourceEncoding</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flink.version</span><span class="token punctuation">&gt;</span></span>1.12.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flink.version</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>java.version</span><span class="token punctuation">&gt;</span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>java.version</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scala.binary.version</span><span class="token punctuation">&gt;</span></span>2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scala.binary.version</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>maven.compiler.source</span><span class="token punctuation">&gt;</span></span>${java.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>maven.compiler.source</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>maven.compiler.target</span><span class="token punctuation">&gt;</span></span>${java.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>maven.compiler.target</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>flink-java<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>${flink.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>provided<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>flink-streaming-java_${scala.binary.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>${flink.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>provided<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.slf4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>slf4j-api<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.7.15<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>compile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.slf4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>slf4j-log4j12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.7.7<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>runtime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.2.17<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>runtime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">&gt;</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>coding</li></ul><p>这里我们稍微改造一下，我们用 netcat 往某个端口中传输数据，这样就可以模拟源源不断的数据流，让 wordcount 程序监听这个端口并进行单词计数，并且让最终的结果输出到 log 中。代码如下:</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">package com.bruce.wordcount;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction;import org.apache.flink.util.Collector;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class WordCount {    static Logger logger = LoggerFactory.getLogger(WordCount.class);    /**     * String --&gt; Tuple2&lt;String, Integer&gt;     *     * Implements the string tokenizer that splits sentences into words as a user-defined     * FlatMapFunction. The function takes a line (String) and splits it into multiple pairs in the     * form of "(word,1)" ({@code Tuple2&lt;String, Integer&gt;}).     */    public static final class Tokenizer            implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {        @Override        public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {            // normalize and split the line            String[] tokens = value.toLowerCase().split("\\W+");            // emit the pairs            for (String token : tokens) {                if (token.length() &gt; 0) {                    out.collect(new Tuple2&lt;&gt;(token, 1));                }            }        }    }    public static void main(String[] args) throws Exception {        //参数解析        if (args.length != 2) {            logger.error("Usage: \n");            logger.error("Please input host and port.");            return;        }        String host = args[0];        int port = Integer.parseInt(args[1]);        // set up the execution environment        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // source        DataStream&lt;String&gt; source = env.addSource(new SocketTextStreamFunction(host, port, "\n", 0)).name("Source");        // transform        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts =                // split up the lines in pairs (2-tuples) containing: (word,1)                source.flatMap(new Tokenizer())                        // group by the tuple field "0" and sum up tuple field "1"                        .keyBy(value -&gt; value.f0)                        .sum(1).name("Transform");        // sink = log        // 这里为了方便展示效果，将结果直接输出到 log        counts.addSink(new WordCountSink()).name("Sink");        // execute program        env.execute("WordCount from socket by bruce.");    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>改造的 Sink 代码：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>bruce<span class="token punctuation">.</span>wordcount</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>tuple<span class="token punctuation">.</span></span><span class="token class-name">Tuple2</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>sink<span class="token punctuation">.</span></span><span class="token class-name">SinkFunction</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>slf4j<span class="token punctuation">.</span></span><span class="token class-name">Logger</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>slf4j<span class="token punctuation">.</span></span><span class="token class-name">LoggerFactory</span><span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCountSink</span> <span class="token keyword">implements</span> <span class="token class-name">SinkFunction</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Tuple2</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">&gt;</span><span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">long</span> serialVersionUID <span class="token operator">=</span> <span class="token number">1L</span><span class="token punctuation">;</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token class-name">Logger</span> logger <span class="token operator">=</span> <span class="token class-name">LoggerFactory</span><span class="token punctuation">.</span><span class="token function">getLogger</span><span class="token punctuation">(</span><span class="token class-name">WordCountSink</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">invoke</span><span class="token punctuation">(</span><span class="token class-name">Tuple2</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">&gt;</span></span> value<span class="token punctuation">,</span> <span class="token class-name">Context</span> context<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">{</span>        logger<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"{ Word: \""</span><span class="token operator">+</span> value<span class="token punctuation">.</span>f0 <span class="token operator">+</span> <span class="token string">"\", Cnt:"</span> <span class="token operator">+</span> value<span class="token punctuation">.</span>f1 <span class="token operator">+</span><span class="token string">"}"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>打包</li></ul><p>进入项目目录，使用 mvn 命令打包</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mvn clean package -Dmaven.test.skip<span class="token operator">=</span>true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624009747126-image.png" alt="打包"></p><ul><li>开启监听端口</li></ul><p>提交任务前需要先开启监听端口，否则会报链接失败的错误，再开一个终端执行以下命令：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">nc</span> -l <span class="token number">10000</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意，这里窗口会阻塞</p><h4 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h4><p>进入如下目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1/bin<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行提交命令 (jar 包路径最好用绝对路径)</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">flink run -c com.wordcount.WordCount /xxx/IdeaProjects/FlinkPractice/target/FlinkPractice-1.0-SNAPSHOT.jar localhost <span class="token number">10000</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010135373-image.png"></p><ul><li>查看任务</li></ul><p>实时流处理任务肯定是一直在 running 的，因为需要处理源源不断的数据。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010195930-image.png" alt="任务提交成功"></p><ul><li>输入数据</li></ul><p>在阻塞窗口中输入一些单词，回车就会被发送出去。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010294222-image.png" alt="输入数据"></p><h4 id="查看任务日志"><a href="#查看任务日志" class="headerlink" title="查看任务日志"></a>查看任务日志</h4><p>Task Manager 里面查看日志，如图则表示统计成功。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010394704-image.png" alt="查看日志"></p><h4 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h4><p>完整代码已经放在 github</p><p><a href="https://github.com/BruceWong96/Flink-Guide">https://github.com/BruceWong96/Flink-Guide</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink 实践篇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> 入门 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
