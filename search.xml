<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2021/08/18/java-xiang-ying-shi-kuang-jia-reactor-zhong-de-mono-he-flux/"/>
      <url>/2021/08/18/java-xiang-ying-shi-kuang-jia-reactor-zhong-de-mono-he-flux/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/08/13/she-ji-mo-shi/"/>
      <url>/2021/08/13/she-ji-mo-shi/</url>
      
        <content type="html"><![CDATA[<h4 id="1-设计原则"><a href="#1-设计原则" class="headerlink" title="1. 设计原则"></a>1. 设计原则</h4><h5 id="1-1-单一职责原则（SRP）"><a href="#1-1-单一职责原则（SRP）" class="headerlink" title="1.1 单一职责原则（SRP）"></a>1.1 单一职责原则（SRP）</h5><blockquote><p>A class or module should have a single responsibility。</p></blockquote><p>不要设计大而全的类，要设计粒度小、功能单一的类。</p><ul><li><p><strong>如何判断类的职责是否足够单一？</strong></p><p>不同的应用场景、不同阶段的需求背景、不同的业务层面，对同一个类的职责是否单一，可能会有不同的判定结果。实际上，一些侧面的判断指标更具有指导意义和可执行性，比如，出现下面这些情况就有可能说明这类的设计不满足单一职责原则：</p><ul><li>类中的代码行数、函数或者属性过多；</li><li>类依赖的其他类过多，或者依赖类的其他类过多；</li><li>私有方法过多；</li><li>比较难给类起一个合适的名字；</li><li>类中大量的方法都是集中操作类中的某几个属性。</li></ul></li><li><p><strong>类的职责是否设计得越单一越好？</strong></p><ul><li>单一职责原则通过避免设计大而全的类，避免将不相关的功能耦合在一起，来提高类的内聚性。同时，类职责单一，类依赖的和被依赖的其他类也会变少，减少了代码的耦合性，以此来实现代码的高内聚、低耦合。但是，如果拆分得过细，实际上会适得其反，反倒会降低内聚性，也会影响代码的可维护性。</li></ul></li></ul><p><strong>场景题</strong></p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">UserInfo</span> <span class="token punctuation">{</span>  <span class="token keyword">private</span> <span class="token keyword">long</span> userId<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> username<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> email<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> telephone<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token keyword">long</span> createTime<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token keyword">long</span> lastLoginTime<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> avatarUrl<span class="token punctuation">;</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> provinceOfAddress<span class="token punctuation">;</span> <span class="token comment">// 省</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> cityOfAddress<span class="token punctuation">;</span> <span class="token comment">// 市</span>  <span class="token keyword">private</span> <span class="token class-name">String</span> regionOfAddress<span class="token punctuation">;</span> <span class="token comment">// 区 </span>  <span class="token keyword">private</span> <span class="token class-name">String</span> detailedAddress<span class="token punctuation">;</span> <span class="token comment">// 详细地址</span>  <span class="token comment">// ...省略其他属性和方法...</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>UserInfo 类是否满足单一职责原则？</p><p>实际上，要从中做出选择，我们不能脱离具体的应用场景。如果在这个社交产品中，用户的地址信息跟其他信息一样，只是单纯地用来展示，那 UserInfo 现在的设计就是合理的。但是，如果这个社交产品发展得比较好，之后又在产品中添加了电商的模块，用户的地址信息还会用在电商物流中，那我们最好将地址信息从 UserInfo 中拆分出来，独立成用户物流信息（或者叫地址信息、收货信息等）。</p><p>我们再进一步延伸一下。如果做这个社交产品的公司发展得越来越好，公司内部又开发出了很多其他产品（可以理解为其他 App）。公司希望支持统一账号系统，也就是用户一个账号可以在公司内部的所有产品中登录。这个时候，我们就需要继续对 UserInfo 进行拆分，将跟身份认证相关的信息（比如，email、telephone 等）抽取成独立的类。</p><p><strong>总结：不同的应用场景、不同阶段的需求背景下，对同一个类的职责是否单一的判定，可能都是不一样的</strong>。</p><h5 id="1-2-开闭原则（OCP）"><a href="#1-2-开闭原则（OCP）" class="headerlink" title="1.2 开闭原则（OCP）"></a>1.2 开闭原则（OCP）</h5><blockquote><p>Software entities (modules, classes, functions, etc.) should be open for extension , but closed for modification。</p></blockquote><p>软件实体（模块、类、方法等）应该“对扩展开放、对修改关闭”。</p><ul><li><p><strong>如何理解“对扩展开放、对修改关闭”？</strong></p><p>添加一个新的功能，应该是通过在已有代码基础上扩展代码（新增模块、类、方法、属性等），而非修改已有代码（修改模块、类、方法、属性等）的方式来完成。关于定义，我们有两点要注意。第一点是，开闭原则并不是说完全杜绝修改，而是以最小的修改代码的代价来完成新功能的开发。第二点是，同样的代码改动，在粗代码粒度下，可能被认定为“修改”；在细代码粒度下，可能又被认定为“扩展”。</p></li><li><p><strong>如何做到“对扩展开放、修改关闭”？</strong></p><p>我们要时刻具备扩展意识、抽象意识、封装意识。在写代码的时候，我们要多花点时间思考一下，这段代码未来可能有哪些需求变更，如何设计代码结构，事先留好扩展点，以便在未来需求变更的时候，在不改动代码整体结构、做到最小代码改动的情况下，将新的代码灵活地插入到扩展点上。</p><p>很多设计原则、设计思想、设计模式，都是以提高代码的扩展性为最终目的的。特别是 23 种经典设计模式，大部分都是为了解决代码的扩展性问题而总结出来的，都是以开闭原则为指导原则的。最常用来提高代码扩展性的方法有：多态、依赖注入、基于接口而非实现编程，以及大部分的设计模式（比如，装饰、策略、模板、职责链、状态）。</p></li></ul><h5 id="1-3-里式替换（LSP）"><a href="#1-3-里式替换（LSP）" class="headerlink" title="1.3 里式替换（LSP）"></a>1.3 里式替换（LSP）</h5><blockquote><p>Functions that use pointers of references to base classes must be able to use objects of derived classes without knowing it。</p></blockquote><p>子类对象（object of subtype/derived class）能够替换程序（program）中父类对象（object of base/parent class）出现的任何地方，并且保证原来程序的逻辑行为（behavior）不变及正确性不被破坏。</p><p>里式替换原则是用来指导，继承关系中子类该如何设计的一个原则。理解里式替换原则，最核心的就是理解“design by contract，按照协议来设计”这几个字。父类定义了函数的“约定”（或者叫协议），那子类可以改变函数的内部实现逻辑，但不能改变函数原有的“约定”。这里的约定包括：函数声明要实现的功能；对输入、输出、异常的约定；甚至包括注释中所罗列的任何特殊说明。</p><p>理解这个原则，我们还要弄明白里式替换原则跟多态的区别。虽然从定义描述和代码实现上来看，多态和里式替换有点类似，但它们关注的角度是不一样的。多态是面向对象编程的一大特性，也是面向对象编程语言的一种语法。它是一种代码实现的思路。而里式替换是一种设计原则，用来指导继承关系中子类该如何设计，子类的设计要保证在替换父类的时候，不改变原有程序的逻辑及不破坏原有程序的正确性。</p><p>里氏替换就是子类完美继承父类的设计初衷，并做了增强。</p><h5 id="1-4-接口隔离原则（ISP）"><a href="#1-4-接口隔离原则（ISP）" class="headerlink" title="1.4 接口隔离原则（ISP）"></a>1.4 接口隔离原则（ISP）</h5><blockquote><p>Clients should not be forced to depend upon interfaces that they do not use。</p></blockquote><p>客户端不应该被强迫依赖它不需要的接口。其中的“客户端”，可以理解为接口的调用者或者使用者。</p><ul><li><p>如何理解“接口隔离原则”？</p><p>理解“接口隔离原则”的重点是理解其中的“接口”二字。这里有三种不同的理解。</p><p>如果把“接口”理解为一组接口集合，可以是某个微服务的接口，也可以是某个类库的接口等。如果部分接口只被部分调用者使用，我们就需要将这部分接口隔离出来，单独给这部分调用者使用，而不强迫其他调用者也依赖这部分不会被用到的接口。</p><p>如果把“接口”理解为单个 API 接口或函数，<strong>部分调用者只需要函数中的部分功能</strong>，那我们就需要把函数拆分成粒度更细的多个函数，让调用者只依赖它需要的那个细粒度函数。</p><p>如果把“接口”理解为 OOP 中的接口，也可以理解为面向对象编程语言中的接口语法。那接口的设计要尽量单一，不要让接口的实现类和调用者，依赖不需要的接口函数。</p></li></ul><h5 id="1-5-依赖反转原则（DIP）"><a href="#1-5-依赖反转原则（DIP）" class="headerlink" title="1.5 依赖反转原则（DIP）"></a>1.5 依赖反转原则（DIP）</h5><blockquote><p>High-level modules shouldn’t depend on low-level modules. Both modules should depend on abstractions. In addition, abstractions shouldn’t depend on details. Details depend on abstractions.</p></blockquote><p>高层模块（high-level modules）不要依赖低层模块（low-level）。高层模块和低层模块应该通过抽象（abstractions）来互相依赖。除此之外，抽象（abstractions）不要依赖具体实现细节（details），具体实现细节（details）依赖抽象（abstractions）。</p><p>所谓高层模块和低层模块的划分，简单来说就是，在调用链上，调用者属于高层，被调用者属于低层。</p><p><strong>场景</strong></p><p>Tomcat 是运行 Java Web 应用程序的容器。我们编写的 Web 应用程序代码只需要部署在 Tomcat 容器下，便可以被 Tomcat 容器调用执行。按照之前的划分原则，Tomcat 就是高层模块，我们编写的 Web 应用程序代码就是低层模块。Tomcat 和应用程序代码之间并没有直接的依赖关系，两者都依赖同一个“抽象”，也就是 Servlet 规范。Servlet 规范不依赖具体的 Tomcat 容器和应用程序的实现细节，而 Tomcat 容器和应用程序依赖 Servlet 规范。</p><ul><li><p>控制反转</p><p>实际上，控制反转是一个比较笼统的设计思想，并不是一种具体的实现方法，一般用来指导框架层面的设计。这里所说的“控制”指的是对程序执行流程的控制，而“反转”指的是在没有使用框架之前，程序员自己控制整个程序的执行。在使用框架之后，整个程序的执行流程通过框架来控制。流程的控制权从程序员“反转”给了框架。</p></li><li><p>依赖注入</p><p>依赖注入和控制反转恰恰相反，它是一种具体的编码技巧。我们不通过 new 的方式在类内部创建依赖类的对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类来使用。</p></li><li><p>依赖注入框架</p><p>我们通过依赖注入框架提供的扩展点，简单配置一下所有需要的类及其类与类之间依赖关系，就可以实现由框架来自动创建对象、管理对象的生命周期、依赖注入等原本需要程序员来做的事情。</p></li><li><p>依赖反转原则</p><p>依赖反转原则也叫作依赖倒置原则。这条原则跟控制反转有点类似，主要用来指导框架层面的设计。高层模块不依赖低层模块，它们共同依赖同一个抽象。抽象不要依赖具体实现细节，具体实现细节依赖抽象。</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基于 Apache Spark SQL 的交互式查询在 Pinterest 的实践</title>
      <link href="/2021/08/12/ji-yu-apache-spark-sql-de-jiao-hu-shi-cha-xun-zai-pinterest-de-shi-jian/"/>
      <url>/2021/08/12/ji-yu-apache-spark-sql-de-jiao-hu-shi-cha-xun-zai-pinterest-de-shi-jian/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文作者：<br>Sanchay Javeria | Software Engineer, Big Data Query Platform, Data Engineering<br>Ashish Singh | Technical Lead, Big Data Query Platform, Data Engineering</p></blockquote><h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><ul><li>为什么要基于 Spark SQL 做交互式查询，Presto 不是更好的选择吗？</li></ul><p>查询平台的用户都是与业务有关的，数仓、分析师、业务开发、运营等等，面向的对象不同，使用的引擎也就不同，Presto 的交互式查询体验好，都可以满足，但是日常工作中，数仓和分析师同学都需要写 Spark SQL 脚本，那么就需要有个地方来验证脚本，基于 Spark SQL 的交互式查询就是用来满足这个需求的理想方案。</p><p>Presto 查询是有限规模的快速查询，而 Spark SQL 是用来支持全规模的查询。</p><p>—下面是正文—</p><hr><p>为了实现我们的使命，即通过我们的视觉发现引擎为每个人带来灵感，Pinterest 严重依赖于制定数据驱动的决策，以改善超过 4.75 亿月活跃用户的 Pinner 体验。可靠、快速和可扩展的交互式查询对于使这些数据驱动的决策至关重要。过去，我们发布了 <a href="https://medium.com/pinterest-engineering/presto-at-pinterest-a8bda7515e52" title="Presto">Pinterest 的 Presto</a> 如何提供此功能。在这里，我们将分享我们如何构建一个可扩展、可靠且高效的交互式查询平台，该平台每天使用 Apache Spark SQL 处理数百 PB 的数据。通过对各种架构选择、过程中的挑战以及我们对这些挑战的解决方案的详细讨论，我们分享了如何使用 Spark SQL 成功地进行交互式查询。</p><h4 id="计划-Scheduled-查询与交互式-Interactive-查询"><a href="#计划-Scheduled-查询与交互式-Interactive-查询" class="headerlink" title="计划 (Scheduled) 查询与交互式 (Interactive) 查询"></a>计划 (Scheduled) 查询与交互式 (Interactive) 查询</h4><p>查询是用户从 Pinterest 数据中获得理解的最流行方式。此类分析的应用存在于所有业务/工程功能中，例如机器学习、广告、搜索、家庭供稿推荐、信任与安全等。提交这些查询主要有两种方式：计划和交互式。</p><ol><li>计划查询是按预定义节奏运行的查询，这些查询通常具有严格的服务级别目标 (SLO)。</li><li>交互式查询是在需要时执行的查询，通常不会以预定义的节奏重复。与预定查询不同，用户等待交互式查询完成并且不知道可能导致查询失败的潜在问题。这些特点使得交互式查询平台的需求不同于预定查询平台。</li></ol><p>以下部分，我们将深入探讨如何在 Pinterest 上使用 Spark SQL 扩展交互式查询。我们首先讨论如何在 Pinterest 上使用 Spark SQL 以及特定于使用 Spark SQL 进行交互式查询的挑战。我们通过介绍架构进行跟进，并讨论我们如何解决我们在此过程中面临的挑战。</p><h4 id="使用-Spark-SQL-进行交互式查询"><a href="#使用-Spark-SQL-进行交互式查询" class="headerlink" title="使用 Spark SQL 进行交互式查询"></a>使用 Spark SQL 进行交互式查询</h4><p>我们支持 Hive、Presto 和 Spark SQL 来查询数据。但是，我们正在弃用 Hive 以支持 Spark SQL，这给我们留下了两个主要的查询引擎（即 Presto 和 Spark SQL）。Presto 用于快速交互查询。Spark SQL 用于所有计划查询（在 Hive 弃用完成后不久）和对大型数据集的交互式查询。以下是我们在从 Hive 迁移到 Spark SQL 时考虑支持使用 Spark SQL 进行交互式查询的各种方法。</p><h5 id="Apache-Spark-的-Thrift-JDBC-ODBC-服务器"><a href="#Apache-Spark-的-Thrift-JDBC-ODBC-服务器" class="headerlink" title="Apache Spark 的 Thrift JDBC/ODBC 服务器"></a>Apache Spark 的 Thrift JDBC/ODBC 服务器</h5><p>Apache Spark 的 Thrift JDBC/ODBC Server (STS) 类似于 HiveServer2，允许客户端通过 JDBC/ODBC 协议执行 Spark SQL 查询。JDBC/ODBC 协议是各种客户端提交查询的最流行方式之一。使用 STS 将允许现有的 JDBC/ODBC 协议支持工具与 Spark SQL 无缝协作。然而，这种方法并没有在提交到同一个 thrift 服务器的查询之间提供适当的隔离。</p><p>单个查询的问题可能会影响在同一节点服务器上运行的所有其他查询。过去使用 Hiveserver2 进行交互式查询时，我们看到了几个问题，即错误的查询导致整个服务器宕机，导致所有并发运行的查询终止/失败。大多数情况下，它要么是由于在本地模式下运行的单个查询在查询优化中占用过多内存，要么是由于查询加载了本地 jar 导致服务器内核崩溃。根据我们的经验，决定不选择这种方法。</p><h5 id="Spark-SQL-查询作为-Apache-YARN-上的-shell-命令应用程序"><a href="#Spark-SQL-查询作为-Apache-YARN-上的-shell-命令应用程序" class="headerlink" title="Spark SQL 查询作为 Apache YARN 上的 shell 命令应用程序"></a>Spark SQL 查询作为 Apache YARN 上的 shell 命令应用程序</h5><p>运行 Spark SQL 查询的另一种常见机制是通过 spark-sql 命令行界面 (CLI)。但是，CLI 方法不适用于交互式应用程序，并且不能提供最佳用户体验。</p><p>可以在我们的 YARN 集群上从各种客户端构建一个服务，该服务将 spark-sql CLI 作为 shell 命令应用程序启动。但是，这会导致在 YARN 集群上等待容器分配，然后为每个查询启动一个 Spark 会话的前期成本。此过程最多可能需要几分钟，具体取决于集群上的资源可用性。</p><p>这种方法会导致交互式查询的用户体验不佳，例如，因为用户需要等待几分钟才能找到语法问题。此外，这种方法使得检索结果、提供语句级进度更新或在发生故障时从驱动程序日志中获取异常堆栈跟踪变得困难。这些是我们对出色的交互式查询体验的一些要求。</p><h5 id="Apache-Livy-与批处理会话"><a href="#Apache-Livy-与批处理会话" class="headerlink" title="Apache Livy 与批处理会话"></a>Apache Livy 与批处理会话</h5><p>Apache Livy 是一种服务，可通过 RESTful 接口与 Spark 集群进行交互。使用 Livy，我们可以轻松地将 Spark SQL 查询提交到我们的 YARN 集群，并通过简单的 REST 调用管理 Spark 上下文。这是对我们复杂的 Spark 基础架构的理想抽象，并允许与面向用户的客户端直接集成。</p><p>Livy 提供两种作业提交选项：批处理和交互式。批处理模式类似于用于提交批处理应用程序的 spark-submit。在批处理模式下，查询的所有语句一起提交执行。这使得我们为交互式查询设想的一些可用性功能变得困难，例如：根据语句对在哪里运行查询做出不同的选择，支持使用 SQL 语句更改 spark 会话的功能，以及创建可重用的用户会话/缓存. 我们将在本文后面详细讨论这些功能。</p><h5 id="Apache-Livy-与交互式会话"><a href="#Apache-Livy-与交互式会话" class="headerlink" title="Apache Livy 与交互式会话"></a>Apache Livy 与交互式会话</h5><p>与 Apache Livy 的批处理会话不同，交互式会话使我们能够启动会话，将查询和/或语句作为单独的请求提交，并在完成时明确结束会话。</p><p>此外，Livy 通过会话恢复和故障隔离提供多租户、高可用性，这些都是我们的首要架构优先事项。这帮助我们选择 Livy 作为 Pinterest 交互式 Spark SQL 查询的理想解决方案。</p><h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p>下面的图 1 概述了 Spark SQL 的查询执行架构和交互式查询用例的请求流。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628739720821-architecture.png" alt="图 1 在 Pinterest 上使用 Spark SQL 进行预定和交互式查询的请求流"></p><p>该图提出的一个明显问题是为什么我们需要分别处理 DDL 和 DML 查询。我们将在后面讨论，同时讨论我们在使用 Spark SQL 进行交互式查询时所面临的挑战以及我们如何解决这些挑战。下面针对交互式 DML 和 DDL 查询详细说明图 1 中的控制流。</p><h5 id="交互式-DML-查询"><a href="#交互式-DML-查询" class="headerlink" title="交互式 DML 查询"></a>交互式 DML 查询</h5><ol><li>Querybook 和 Jupyter 等客户端向 Livy 提交交互式 DML 查询；</li><li>Livy 从 YARN 资源管理器 (RM) 请求容器以运行远程 Spark 上下文 (RSC) 客户端；</li><li>RM 分配一个容器，在其中启动 RSC 客户端。此 RSC 客户端然后启动 RSC 驱动程序；</li><li>Livy 通过与运行驱动程序的 RSC 客户端通信来跟踪查询进度；</li><li>Spark SQL 驱动程序根据需要从 Hive Metastore Service (HMS) 获取查询计划所需的表元数据；</li><li>根据资源需求，驱动程序向 RM 请求容器以启动执行程序；</li><li>Spark SQL 驱动程序在执行器之间分配任务和协调工作，直到为用户查询完成所有 Spark 作业。</li></ol><h5 id="交互式-DDL-查询"><a href="#交互式-DDL-查询" class="headerlink" title="交互式 DDL 查询"></a>交互式 DDL 查询</h5><ol><li>Client 向 Livy 提交交互式 DDL 查询。</li><li>Livy 从本地会话池中获取 Spark 会话（详细信息将在后面的部分中讨论），并正确更新当前请求用户的用户凭据。</li><li>本地 Spark SQL 驱动从 HMS 获取用于查询规划的表元数据，并根据需要执行 DDL 操作。</li></ol><h4 id="挑战和我们的解决方案"><a href="#挑战和我们的解决方案" class="headerlink" title="挑战和我们的解决方案"></a>挑战和我们的解决方案</h4><p>本节讨论我们必须解决的各种挑战，以便在 Pinterest 上使用 Spark SQL 进行交互式查询。</p><h5 id="无缝查询提交"><a href="#无缝查询提交" class="headerlink" title="无缝查询提交"></a>无缝查询提交</h5><p>虽然 Livy 提供了一种可靠的解决方案来将查询作为 Spark 作业提交，但我们需要用户使用标准接口从任何客户端提交查询，该接口可用作插入式依赖项以轻松与 Livy 通信。</p><p>我们在 Livy 之上构建了一个通用的 DB-API 兼容 Python 客户端，称为 BigPy，多个查询客户端使用它来提交查询。在 BigPy 中，我们提供了一个接口来实现以下功能：</p><ul><li>状态轮询：它监视 Livy 会话的状态并向客户端报告应用程序是否成功、失败或当前正在运行。此外，我们还报告了 Spark 应用程序的完成百分比；</li><li>跟踪链接：返回所有跟踪链接，用于监控 Spark 应用程序的状态，包括指向 Spark UI、驱动程序日志和 Dr.Elephant 的链接，用于监控 Spark 应用程序的性能和调优；</li><li>结果检索：它提供了从 AWS S3 等对象存储中以分页方式检索查询结果的能力；</li><li>异常检索：Spark 驱动程序和执行程序日志通常很嘈杂，查找查询失败的原因可能很麻烦。BigPy 返回异常，其堆栈跟踪直接到客户端，以获得更轻松的调试体验。</li></ul><p>BigPy 启用了跨多个不同系统与 Livy 交互的模块化方式，提供了关注点与客户端代码的明确分离。</p><h5 id="快速元数据查询"><a href="#快速元数据查询" class="headerlink" title="快速元数据查询"></a>快速元数据查询</h5><p>spark-shell 程序以 cluster mode 向 RM 发送 YARN 应用程序请求。RM 启动 Application Master (AM)，然后启动驱动程序。驱动程序进一步向 RM 请求用于启动执行程序的更多容器。我们发现，这个资源分配过程可能需要几分钟才能开始处理每个查询，这会显着增加数据定义语言 (DDL)/仅元数据查询的延迟，这些查询通常是低延迟的元数据操作。</p><p>DDL 查询在驱动程序上执行，不需要额外的执行程序或与 DML 查询相同数量的隔离。为了缓解 YARN 集群上容器分配的冗余延迟和 SparkSession 启动时间的问题，我们在 Apache Livy 中实现了一个本地会话池，它维护了一个以本地模式运行的 Spark 会话池。</p><p>这个问题有两个部分：1) 将查询识别为 DDL 语句，以及 2) 实现 Spark 应用程序的缓存池来处理这些查询。我们利用 SparkSqlParser 为用户查询获取逻辑计划以识别 DDL 查询。由于这个逻辑计划只是一个继承自 TreeNode 类的逻辑运算符树，我们可以轻松地遍历这棵树并根据一组 DDL 执行命令检查每个节点的类。如果逻辑计划的所有节点都与 DDL 命令匹配，我们将查询标识为 DDL。在实践中，它看起来像这样：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628746567392-image.png"></p><p>一旦我们知道查询是 DDL 语句，我们就将其路由到缓存的 Spark 应用程序之一。我们在 Livy 中构建了这个缓存的应用程序池，由本地运行的 Spark 驱动程序池表示。它旨在完全自力更生，具有以下功能：</p><ul><li>过时应用程序的自动垃圾收集并启动新应用程序</li><li>一个守护线程监控池的健康状况并将查询路由到下一个可用的应用程序</li><li>以可配置的节奏重新启动应用程序，以确保它获取最新的资源（例如架构 jar）以确保数据新鲜度</li><li>在开始时异步启动轻量级元数据操作以初始化 SparkContext 并建立与 Metastore 的实时连接以加快后续操作</li></ul><p>通过这种设计，我们将查询延迟从 70 秒减少到平均 10 秒（约 6.3 倍的改进）。</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628746645987-image.png" alt="图 2：在 local 与 cluster mode 下运行的 DDL 查询的时间比较"></p><h5 id="快速失败：更快的语法检查"><a href="#快速失败：更快的语法检查" class="headerlink" title="快速失败：更快的语法检查"></a>快速失败：更快的语法检查</h5><p>在 cluster mode 下运行每个查询的另一个缺点是语法检查将至多花费在最坏情况下启动应用程序所需的时间。在临时环境中，用户通常希望更早地出现语法问题，等待几分钟才报告语法问题会带来令人失望的体验。我们通过使用 SparkSqlParser 改进了这一点，并在启动 YARN 应用程序之前获取查询的逻辑计划。如果查询包含语法错误，解析器将在生成逻辑计划时抛出“ParseException”，并方便地返回行号和列号，我们将其报告给客户端。通过这种方式，我们将整体语法检查延迟从几分钟减少到不到两秒（改进了 30 倍以上）。</p><h5 id="错误处理建议"><a href="#错误处理建议" class="headerlink" title="错误处理建议"></a>错误处理建议</h5><p>查询失败在临时环境中是隐含的。但是，修复这些故障通常是一个艰巨的循环，需要浏览驱动程序日志、通过自我诊断或寻求外部帮助找到解决方案，然后重试查询。为了简化此过程，我们提供了一些常见问题的自动故障排除信息，这些问题乍一看很难修复。此解决方案有四个部分：</p><ol><li>根据上次查询的执行状态判断 YARN 申请失败</li></ol><p>cluster mode 下 Livy Interactive Sessions 的一个问题是它们始终向 YARN AM 报告“成功”状态。发生这种情况是因为 Livy 提交给 SparkLauncher 的远程驱动程序启动了一个 Spark 上下文，在该上下文中运行一些查询，然后关闭该上下文。无论查询运行的状态如何，报告的最终状态始终是 SparkContext 是否能够成功关闭。这会误导用户和平台所有者。为了缓解这个问题，我们在单个交互式会话中跟踪最终查询运行的状态，并在查询失败时在远程驱动程序中抛出运行时异常。这有助于将状态正确报告回 AM 并使用故障原因（如果有）填充 YARN 诊断。</p><ol start="2"><li>识别用户查询中的常见错误</li></ol><p>一旦我们使用查询的失败原因正确填充 YARN 诊断，我们就会利用添加到 YARN 集群的额外日志记录来方便地跟踪 Spark SQL 表中遇到的错误。然后，我们查看了失败堆栈跟踪的历史记录，并使用正则表达式对它们进行了分类。根据频率，我们获得了一个 top-n 错误列表。<br>我们利用 Dr.Elephant 跟踪 Spark 应用程序启发式和指标，并添加了错误分类机制，该机制查看应用程序的 YARN 诊断信息并基于正则表达式引擎对其进行分类。使用上述正则表达式，我们将通过 REST API 公开的常见错误的故障排除信息添加到 Dr.Elephant Web UI 和其他外部客户端（如 Querybook）。</p><ol start="3"><li>Livy 集成 Dr.Elephant</li></ol><p>我们在 Livy 中为每个启动的 Spark 应用程序集成了上面提到的 Dr.Elephant API。此端点在每次查询运行时返回给客户端，便于查看故障排除信息。</p><ol start="4"><li>客户端集成</li></ol><p>从 Livy 获取 Dr.Elephant 故障排除分析端点后，客户端从 API 中提取此信息并将其显示在查询日志中。这样，当我们看到查询失败时，我们可以提供常见错误的故障排除信息，帮助用户更快地诊断问题。</p><h5 id="资源利用可见性"><a href="#资源利用可见性" class="headerlink" title="资源利用可见性"></a>资源利用可见性</h5><p>查看我们的临时集群的历史内存消耗指标，我们注意到应用程序经常过度分配执行程序和驱动程序内存，导致不必要的资源浪费。另一方面，对于内存不足 (OOM) 的应用程序，我们的用户经常要求我们让他们更容易抢先捕获这些问题，以便更快地重新调整他们的查询。</p><p>为了解决这个问题，我们直接在客户端上显示实时内存消耗信息，所有执行程序使用不同的聚合，如最大、最小和平均内存。我们还标记消费不足和过度消费，并根据启发式提示用户采取行动。</p><p>我们使用 <a href="https://spark.apache.org/docs/latest/monitoring.html#metrics" title="Spark 指标库">Spark 指标库</a> 的自定义指标接收器为每个 Spark 应用程序收集实时内存消耗信息。然后我们在 BigPy 中使用这些指标并检查它们是否违反了任何资源阈值，以 UI 友好的降价表格式将信息返回给客户端。这种方法的一个例子可以在下面 GIF 的 Querybook 上看到：</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-8-12/1628747458010-metrics.gif" alt="图 3：具有各种聚合的实时驱动程序/执行程序内存消耗信息"></p><h5 id="大结果处理和状态跟踪"><a href="#大结果处理和状态跟踪" class="headerlink" title="大结果处理和状态跟踪"></a>大结果处理和状态跟踪</h5><p>默认情况下，Livy 对查询结果集的限制为 1,000 行。增加这个限制并不理想，因为结果集存储在内存中，增加这个限制可能会导致像我们这样的内存限制环境中的大规模问题。为了解决这个问题，我们为每个查询的最终结果实施了 AWS S3 重定向。这样，大型结果集可以以多部分方式上传到 S3，而不会影响服务的整体性能。在客户端，我们稍后检索 REST 响应中返回的最终 S3 输出路径，并以分页方式从 S3 获取结果。这使得检索速度更快，而不会在列出路径对象时冒 S3 超时的风险。此重定向也可在查询级别进行配置，以便如果用户期望查询返回小于 1，我们还提供实时进度更新，这是通过对 Spark SQL 查询的已完成任务和活动任务总数与任务总数进行平均而获得的。可以在上面图 3 的 GIF 中看到预览。</p><h4 id="Livy-操作改进"><a href="#Livy-操作改进" class="headerlink" title="Livy 操作改进"></a>Livy 操作改进</h4><p>我们平均每天会看到大约 1,500 个临时 SparkSQL 查询，为了支持这种负载，我们的系统必须对我们的用户保持健康和可靠。我们对可靠性和稳定性进行了大量改进，使我们能够为 Livy 保持 99.5% 的正常运行时间 SLO。一些关键亮点：</p><ul><li>有效的 Livy 负载平衡</li></ul><p>按照设计，Livy 是一个有状态的 Web 服务。它将会话的状态存储在内存中，如查询运行、每个查询的状态、最终结果等。由于我们的客户端遵循 HTTP 轮询机制来获取这些属性，因此很难在上层添加经典/应用程序负载均衡器。为了解决这个问题，我们通过以循环方式将每个查询路由到最不忙的 Livy 实例，在应用程序级别实现了我们的负载平衡算法。此处，“繁忙度”由在特定 Livy 实例上运行的“活动”会话数定义。这种简单但有效的机制使我们能够在整个车队中更均匀地分配负载。</p><ul><li>指标和日志记录改进</li></ul><p>我们为 Livy 添加了事件监听器支持，其中事件被定义为任何 Livy 活动，包括会话创建和向会话提交语句。我们使用这些侦听器将 JSON 对象记录到本地磁盘以跟踪各种事件。这可以在出现问题时更快地进行调试和使用情况监控。</p><ul><li>指标</li></ul><p>我们还使用 Scalatra Metrics 来跟踪关键服务级别指标，例如健康检查、MAU、用户/查询的 DAU 计数、缓存会话命中率、查询成功率等。这些顶级指标对于跟踪我们集群中的整体临时活动非常重要。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>为了支持使用 SQL 分析和处理数百 PB 的数据，我们正在 Pinterest 上融合 Spark SQL 和 Presto。虽然 Presto 仍然是资源需求有限的快速交互式查询最受欢迎的查询引擎选择，但我们使用 Spark SQL 来支持所有规模的查询。交互式查询用例与计划查询有不同的要求。其中一些功能是无缝查询提交、快速元数据查询、快速语法检查以及更好的调试和调整支持。根据我们对交互式查询的需求以及可用开源解决方案提供的功能，我们决定使用 Apache Livy 构建 Spark SQL 交互式查询平台。但是，Livy 没有立即满足我们的要求，我们添加了各种功能来弥补这一差距。在这篇文章中，我们对我们的架构选择和增强功能进行了推理，以使 Pinterest 的交互式查询取得成功。我们计划将这些更改中的大部分回馈给开源社区。</p><blockquote><p>本文翻译自：<a href="https://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b">https://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大厂实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark SQL </tag>
            
            <tag> 交互式查询 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/06/28/yun-yuan-sheng-shu-ju-zhong-tai-du-shu-bi-ji/"/>
      <url>/2021/06/28/yun-yuan-sheng-shu-ju-zhong-tai-du-shu-bi-ji/</url>
      
        <content type="html"><![CDATA[<h3 id="《云原生数据中台》读书笔记"><a href="#《云原生数据中台》读书笔记" class="headerlink" title="《云原生数据中台》读书笔记"></a>《云原生数据中台》读书笔记</h3><p>大数据源于硅谷，国内数据中台的概念源于阿里巴巴，虽然硅谷没有“数据中台”的叫法，但硅谷的公司早已形成了中台的意识（来源于避免重复造轮子，快速迭代，数据驱动，业务驱动的工程师文化理念）。</p><h4 id="数据中台的思路"><a href="#数据中台的思路" class="headerlink" title="数据中台的思路"></a>数据中台的思路</h4><p>中台提供数据能力的共享和复用，前端业务部门可以快速获得全局的数据洞见及现成的数据工具，快速推出由数据支持的产品。</p><h4 id="数据中台要解决的问题"><a href="#数据中台要解决的问题" class="headerlink" title="数据中台要解决的问题"></a>数据中台要解决的问题</h4><ul><li>各个部门重复开发数据，浪费存储与计算资源；</li><li>数据标准不统一，数据使用成本高；</li><li>业务数据孤岛问题严重，数据利用率低。</li></ul><p>数据规范：连接生产数据的业务部门与消费数据的分析部门的桥梁。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>[Flink 实践篇-1] Flink 1.12.1 实现 WordCount 入门案例</title>
      <link href="/2021/06/26/flink-shi-jian-pian-1-flink-1-12-1-shi-xian-wordcount-ru-men-an-li/"/>
      <url>/2021/06/26/flink-shi-jian-pian-1-flink-1-12-1-shi-xian-wordcount-ru-men-an-li/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-26/1624694497803-flink-home-graphic.png" alt="Flink"></p><h4 id="什么是-WordCount-？"><a href="#什么是-WordCount-？" class="headerlink" title="什么是 WordCount ？"></a>什么是 WordCount ？</h4><p>wordcount 简单来讲就是单词计数，是一般大数据计算框架（Hadoop、Spark、Flink）的入门学习案例，相当于编程语言（Java、Python）中的 HelloWorld 案例，适合刚开始了解 Flink 作业提交流程的同学。</p><h4 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h4><ol><li>JDK 1.8 (必须)</li></ol><pre class="line-numbers language-linux" data-language="linux"><code class="language-linux">~  $ java -versionjava version "1.8.0_291"Java(TM) SE Runtime Environment (build 1.8.0_291-b10)Java HotSpot(TM) 64-Bit Server VM (build 25.291-b10, mixed mode)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>Flink 1.12.1（必须）</li></ol><ul><li>采用 tar 包安装的方式，下载地址如下：</li></ul><p><a href="https://archive.apache.org/dist/flink/flink-1.12.1/">https://archive.apache.org/dist/flink/flink-1.12.1/</a></p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623946032805-image.png" alt="下载 Flink 安装包"></p><ul><li>解压安装包</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev  $ <span class="token function">tar</span> -zxvf flink-1.12.1-bin-scala_2.11.tgz~/flink-dev  $ <span class="token function">ls</span> -ltotal <span class="token number">655424</span>drwxr-xr-x@ <span class="token number">13</span> it  staff        <span class="token number">416</span>  <span class="token number">1</span> <span class="token number">10</span> 08:46 flink-1.12.1-rw-r--r--@  <span class="token number">1</span> it  staff  <span class="token number">334271560</span>  <span class="token number">6</span> <span class="token number">18</span> 00:18 flink-1.12.1-bin-scala_2.11.tgz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>查看目录结构</li></ul><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623975272045-image.png" alt="Flink 目录结构"></p><ul><li><p>配置环境变量</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1  $ <span class="token builtin class-name">pwd</span>/xxx/flink-dev/flink-1.12.1~/flink-dev/flink-1.12.1  $ <span class="token function">vi</span> ~/.zshrc<span class="token comment"># 将下面两行添加到 上述文件中</span><span class="token comment"># 因为我这里用的是 zsh ，根据自己的需要选择文件</span><span class="token comment"># mac 默认是 .bash_profile </span><span class="token builtin class-name">export</span> <span class="token assign-left variable">FLINK_HOME</span><span class="token operator">=</span>/xxx/flink-dev/flink-1.12.1<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$FLINK_HOME</span>/bin<span class="token comment"># 保存之后执行以下命令</span><span class="token builtin class-name">source</span> ~/.zshrc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>验证版本</p></li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1  $ flink --versionVersion: <span class="token number">1.12</span>.1, Commit ID: dc404e2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如上显示，则表示安装成功。</p><h4 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h4><p>学过 一些 Java 的用户应该了解，启动集群脚本一般在 bin 目录下</p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623976416741-image.png" alt="bin 目录下的脚本"></p><ul><li>执行启动集群命令<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1/bin  $ ./start-cluster.shStarting cluster.Starting standalonesession daemon on <span class="token function">host</span> MacBook-Pro.lan.Starting taskexecutor daemon on <span class="token function">host</span> MacBook-Pro.lan.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>– 访问 Flink webUI 界面 (localhost:8081)</p><p>如果输入地址后可以看到以下页面，表示集群启动成功。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623976909945-image.png" alt="输入地址"></p><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1623976953750-image.png" alt="web UI"></p><h4 id="编写-wordcount-项目"><a href="#编写-wordcount-项目" class="headerlink" title="编写 wordcount 项目"></a>编写 wordcount 项目</h4><p>Flink 本地启动了一个集群，接下来就是提交我们的任务，任务使用 Java 语言调用 Flink 的 api 来编写。</p><ul><li>创建项目<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624006853454-WeChat0292626db4546a6b740b2b0bdc12c3a3-min.png" alt="New Project"></li></ul><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624006989673-image.png"></p><ul><li>将以下配置复制到 pom.xml 文件</li></ul><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>properties</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>project.build.sourceEncoding</span><span class="token punctuation">&gt;</span></span>UTF-8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>project.build.sourceEncoding</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flink.version</span><span class="token punctuation">&gt;</span></span>1.12.1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flink.version</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>java.version</span><span class="token punctuation">&gt;</span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>java.version</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scala.binary.version</span><span class="token punctuation">&gt;</span></span>2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scala.binary.version</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>maven.compiler.source</span><span class="token punctuation">&gt;</span></span>${java.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>maven.compiler.source</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>maven.compiler.target</span><span class="token punctuation">&gt;</span></span>${java.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>maven.compiler.target</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>properties</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>flink-java<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>${flink.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>provided<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.flink<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>flink-streaming-java_${scala.binary.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>${flink.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>provided<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.slf4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>slf4j-api<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.7.15<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>compile<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.slf4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>slf4j-log4j12<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.7.7<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>runtime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.2.17<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>runtime<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">&gt;</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>coding</li></ul><p>这里我们稍微改造一下，我们用 netcat 往某个端口中传输数据，这样就可以模拟源源不断的数据流，让 wordcount 程序监听这个端口并进行单词计数，并且让最终的结果输出到 log 中。代码如下:</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">package com.bruce.wordcount;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction;import org.apache.flink.util.Collector;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class WordCount {    static Logger logger = LoggerFactory.getLogger(WordCount.class);    /**     * String --&gt; Tuple2&lt;String, Integer&gt;     *     * Implements the string tokenizer that splits sentences into words as a user-defined     * FlatMapFunction. The function takes a line (String) and splits it into multiple pairs in the     * form of "(word,1)" ({@code Tuple2&lt;String, Integer&gt;}).     */    public static final class Tokenizer            implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {        @Override        public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {            // normalize and split the line            String[] tokens = value.toLowerCase().split("\\W+");            // emit the pairs            for (String token : tokens) {                if (token.length() &gt; 0) {                    out.collect(new Tuple2&lt;&gt;(token, 1));                }            }        }    }    public static void main(String[] args) throws Exception {        //参数解析        if (args.length != 2) {            logger.error("Usage: \n");            logger.error("Please input host and port.");            return;        }        String host = args[0];        int port = Integer.parseInt(args[1]);        // set up the execution environment        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        // source        DataStream&lt;String&gt; source = env.addSource(new SocketTextStreamFunction(host, port, "\n", 0)).name("Source");        // transform        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts =                // split up the lines in pairs (2-tuples) containing: (word,1)                source.flatMap(new Tokenizer())                        // group by the tuple field "0" and sum up tuple field "1"                        .keyBy(value -&gt; value.f0)                        .sum(1).name("Transform");        // sink = log        // 这里为了方便展示效果，将结果直接输出到 log        counts.addSink(new WordCountSink()).name("Sink");        // execute program        env.execute("WordCount from socket by bruce.");    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>改造的 Sink 代码：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>bruce<span class="token punctuation">.</span>wordcount</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>api<span class="token punctuation">.</span>java<span class="token punctuation">.</span>tuple<span class="token punctuation">.</span></span><span class="token class-name">Tuple2</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flink<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>api<span class="token punctuation">.</span>functions<span class="token punctuation">.</span>sink<span class="token punctuation">.</span></span><span class="token class-name">SinkFunction</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>slf4j<span class="token punctuation">.</span></span><span class="token class-name">Logger</span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>slf4j<span class="token punctuation">.</span></span><span class="token class-name">LoggerFactory</span><span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCountSink</span> <span class="token keyword">implements</span> <span class="token class-name">SinkFunction</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Tuple2</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">&gt;</span><span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">long</span> serialVersionUID <span class="token operator">=</span> <span class="token number">1L</span><span class="token punctuation">;</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token class-name">Logger</span> logger <span class="token operator">=</span> <span class="token class-name">LoggerFactory</span><span class="token punctuation">.</span><span class="token function">getLogger</span><span class="token punctuation">(</span><span class="token class-name">WordCountSink</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">invoke</span><span class="token punctuation">(</span><span class="token class-name">Tuple2</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Integer</span><span class="token punctuation">&gt;</span></span> value<span class="token punctuation">,</span> <span class="token class-name">Context</span> context<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">{</span>        logger<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"{ Word: \""</span><span class="token operator">+</span> value<span class="token punctuation">.</span>f0 <span class="token operator">+</span> <span class="token string">"\", Cnt:"</span> <span class="token operator">+</span> value<span class="token punctuation">.</span>f1 <span class="token operator">+</span><span class="token string">"}"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>打包</li></ul><p>进入项目目录，使用 mvn 命令打包</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mvn clean package -Dmaven.test.skip<span class="token operator">=</span>true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624009747126-image.png" alt="打包"></p><ul><li>开启监听端口</li></ul><p>提交任务前需要先开启监听端口，否则会报链接失败的错误，再开一个终端执行以下命令：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">nc</span> -l <span class="token number">10000</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意，这里窗口会阻塞</p><h4 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h4><p>进入如下目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">~/flink-dev/flink-1.12.1/bin<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行提交命令 (jar 包路径最好用绝对路径)</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">flink run -c com.wordcount.WordCount /xxx/IdeaProjects/FlinkPractice/target/FlinkPractice-1.0-SNAPSHOT.jar localhost <span class="token number">10000</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010135373-image.png"></p><ul><li>查看任务</li></ul><p>实时流处理任务肯定是一直在 running 的，因为需要处理源源不断的数据。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010195930-image.png" alt="任务提交成功"></p><ul><li>输入数据</li></ul><p>在阻塞窗口中输入一些单词，回车就会被发送出去。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010294222-image.png" alt="输入数据"></p><h4 id="查看任务日志"><a href="#查看任务日志" class="headerlink" title="查看任务日志"></a>查看任务日志</h4><p>Task Manager 里面查看日志，如图则表示统计成功。<br><img src="https://gitee.com/brucewong96/picture0/raw/master/2021-6-18/1624010394704-image.png" alt="查看日志"></p><h4 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h4><p>完整代码已经放在 github</p><p><a href="https://github.com/BruceWong96/Flink-Guide">https://github.com/BruceWong96/Flink-Guide</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink 实践篇 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 入门 </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
